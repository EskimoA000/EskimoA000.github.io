<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>JMBook Chapter 3 Reading | 2021_timeMachine</title>
  <meta name="author" content="张靖元">
  
  <meta name="description" content="daydayup">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="JMBook Chapter 3 Reading"/>
  <meta property="og:site_name" content="2021_timeMachine"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/EskimoA000.github.io/atom.xml" title="2021_timeMachine" type="application/atom+xml">
  
  
    <link href="/EskimoA000.github.io/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/EskimoA000.github.io/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/EskimoA000.github.io/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  




<meta name="generator" content="Hexo 5.4.0"></head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/EskimoA000.github.io/">2021_timeMachine</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/EskimoA000.github.io/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/EskimoA000.github.io/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/EskimoA000.github.io/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/EskimoA000.github.io/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> JMBook Chapter 3 Reading</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <h2 id="Speech-and-Language-Processing-3rd-ed-draft"><a href="#Speech-and-Language-Processing-3rd-ed-draft" class="headerlink" title="Speech and Language Processing (3rd ed. draft)"></a><a target="_blank" rel="noopener" href="http://www.web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing (3rd ed. draft)</a></h2><h2 id="Chapter-3-：N-gram-Language-Models"><a href="#Chapter-3-：N-gram-Language-Models" class="headerlink" title="Chapter 3 ：N-gram Language Models"></a><a target="_blank" rel="noopener" href="http://www.web.stanford.edu/~jurafsky/slp3/3.pdf">Chapter 3 ：N-gram Language Models</a></h2><h2 id="章节内容综述（笔记）"><a href="#章节内容综述（笔记）" class="headerlink" title="章节内容综述（笔记）"></a>章节内容综述（笔记）</h2><p>N元语法模型（N-gram model）是一个概率模型，可以用来形式化地描述猜测单词的问题。比如要在噪声中或者歧义的输入中辨认出单词，N元语法就是必不可少的，例如应用到输入情况复杂的语音识别（speech recognition）中；同样在写作中拼写校正，语法校正和机器翻译都需要应用N元语法模型；N元语法对于辅助和替代性沟通系统同样重要。<br>   一个N元语法是包含N个单词的序列：二元语法是包含2个单词的序列（如please turn），三元语法是包含3个单词的序列（如 please turn your）。N元语法模型是根据前面出现的单词计算后一个单词的模型。</p>
<p>如果给定了某个单词w的历史h的条件下，我们用条件概率P来计算单词w的概率，来知道这个历史h后面单词w（例如the）的概率有多大。理想情况下，我们可以使用Web这样足够大的语料库，但是会出现三个问题：一是为了很好的估计出概率，我们经常觉得现有的Web规模还不够足够大；二是语言具有创造性，语言总是会不断创造出新的句子来；三是计算量太大，难以计算概率。</p>
<p>为此我们可以计算在一个句子序列中每一个单词都有一个特定的值的联合概率，记为               P(w1,w2,…,wn)，我们可以根据概率的链规则把这个概率分解：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png" alt="img"></p>
<p>我们可以通过把若干的条件概率相乘的办法来估计整个单词序列的联合概率，但是当前面的单词序列长度很大时，我们没有很好的办法来计算某个单词精确的条件概率。所以，我们想到一个处理方法：在计算某个单词的条件概率时，不考虑它之前全部的历史，而是只是考虑最接近该单词的若干个单词，从而近似地逼近该单词的历史。</p>
<p>例如，我们只用前面一个单词的条件概率P，来逼近后面给定的所有单词的概率，这就是二元语法模型，我们有以下的近似逼近公式:</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png" alt="img"></p>
<p>一个单词的概率只依赖它前面单词的概率的这种假设称为马尔可夫假设（Markov assumption）。马尔可夫模型是一种概率模型，我们没有必要查看很远的过去，就可以预见到某一个单位到来的概率。我们可以把二元语法模型推广到三元语法模型，在推广到N元语法模型（看过去的N-1个单词）。</p>
<p>所以在一个序列中，N元语法对于下一个单词的条件概率逼近的公式为：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image006.png" alt="img"></p>
<p>我们再将这概率逼近公式带入之前的概率链规则公式，得到：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image008.png" alt="img"></p>
<p>当我们估计二元语法或N元语法的概率时，我们可以用最简单和最直观的最大自然估计法（Maximum Likelihood Estimation，MLE）。我们可以把从语料库中得到的计数加以归一化（normalize），从而得到N元语法模型参数的MLE估计，进行归一化后，概率都处于0和1之间，以下公式为：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.png" alt="img"></p>
<p>其中C（xy）为计数函数，即单词x和单词y在一起出现的次数。</p>
<p>我们可以加以简化，因为以给定单词Wn-1开头的所有二元语法的计数必定等于单词Wn-1的一元语法的计数：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image012.png" alt="img"></p>
<p>附：用前面符号串的观察频率来除这个特定单词序列的观察频率，就得到N元语法概率的估计值，这个比值称为相对频率（relative frequency）。</p>
<p>N元语法评测：困惑度</p>
<p>评测语言模型性能的最好的方法是把这个语言模型嵌入到某种应用中去，并测试这种应用的总体性能，这种端到端的评测称为外在评测（extrinsic evaluation）。由于端对端的评测常常需要付出很高的代价，我们可以采用内在评测（intrinsic evaluation）度量，即与任何应用无关的模型质量的评测方法。</p>
<p>我们可以把在一些数据上训练，在另一些数据上测试的这种思想加以形式化，把它们分别称为训练集和测试集，或者训练语料库（training corpus）和测试语料库。我们把初始的测试集称为调试测试集，又称为开发集（development test set）。</p>
<p>对于统计模型与测试集匹配的情况，存在一个有用的度量方法，称为困惑度（perplexity）。困惑度（PP）是该语言模型指派给测试集的概率的函数。对于测试集W，困惑度就是用单词数归一化之后的测试集的概率：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image014.png" alt="img"></p>
<p>同样可以使用链规则来展开W的概率：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image016.png" alt="img"></p>
<p>另外一种研究困惑度的办法是语言的加权平均转移因子（weighted average branching factor），转移因子是指语言中的任何一个单词后面可能接续的单词的数量。</p>
<p>​    对那些小数据集的可变性而造成的糟糕的估计结果进行的修正，称为平滑（smoothing），我们将削减一些来自高计数的概率，用它们来填补那些零计数的概率，从而使得概率分布不至于太过于参差不一。</p>
<p>​    一个简单的平滑方法是：取二元语法的计数矩阵，在我们把它们归一化为概率之前，先给所有的计数加一。这种算法称为Laplace平滑(也被称为加一平滑)。Laplace平滑只是对于每一个计数加一，由于词汇表中有V个词，并且每一个词都有了增量，所以我们还有必要来调整分母，以便考虑到这个附加的观察值V：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image018.png" alt="img"></p>
<p>​    还有一种相关的方法是把平滑看成打折（discounting），也就是把某个非零的计数降下来，使得到的概率量可以指派给那些为零的计数。</p>
<p>​    我们可以通过分析得到加一平滑的折扣率太高了，一个很自然的想法是降低从高频词汇那里匀出来的概率，每个词的频数不是加1，而是加上一个小于1的浮点数k，这种方法被称为add-k smoothing：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image020.png" alt="img"></p>
<p>add-k平滑要求我们有选择k的算法，例如我们可以通过在dev集上优化k值。尽管add-k smoothing在一些任务上很有效（包括文本分类的任务），但它应用在语言模型上的效果仍然不是很好，平滑后的频数要么对原频数没什么改变，要么有着不恰当的折扣率。</p>
<p>dd-1平滑和add-k平滑可以解决N元模型的词汇零频数问题，但是N元模型的零值问题还包含另一种情况：假设我们要计算P(wn|wn-2wn-1)，但是我们没有wn-2wn-1wn的词组，我们可以退而求其次，计算二元概率P（wn|wn-1）代替三元概率。类似的，如果我们没有二元概率P（wn|wn-1），可以使用一元概率P(wn)进行代替。</p>
<p>有时在训练的时候少一点上下文可以模型的泛化能力。有两种方法利用这种n元模型之间的继承关系，分别是后退（backoff）和插值（interpolation）。当模型无法估计N元词组时，后退一步选用N-1元模型替代，这种方法称为后退。而插值的方法会利用所有的N元模型信息。</p>
<p>当估计一个三元词组的概率时，加权结合三元模型、二元模型和一元模型的结果。线性插值是相对简单的插值方法，将N个模型的结果做线性组合。利用线性插值估计三元词组的概率时，公式如下：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image022.png" alt="img"></p>
<p>其中所有的λ和为1，∑iλi=1</p>
<p>在使用后退算法计算概率时，如果N元词组计算的概率为0，使用N-1元近似来N元词组的概率。为了维持概率分布的正确性，在后退算法中，需要对higher-order概率折扣处理。如果higher-order的概率不进行折扣处理，而是直接使用lower-order的概率去近似，概率空间会被扩大，概率和将大于1。比如P（wn|wn-1）的概率明显比P(wn)概率小，如果直接用一元概率替代，所有二元概率的概率和将大于1。因此，我们需要用一个函数α来均衡概率分布。</p>
<p>这种有折扣的后退方法被称为Katz backoff。在Katz后退方法中，如果C(wnn-N+1)不为0，那么概率值为折扣概率P∗，如果C(wnn-N+1)是0，就递归地退回N-1的短上下文情境中计算概率并乘以α函数的结果，具体公式如下所示：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image024.png" alt="img"></p>
<p>在现代N元语法平滑中最普遍使用的一种方法是带插值的Kneser-Ney算法。这个算法的根源是一种称为绝对折扣（absolute discounting）的打折方法。</p>
<p>熵是信息的一种度量，可以用户来了解在一个特定的语法中的信息量是多少，度量给定语法和给定语言的匹配程度有多高，预测一个给定的N元语法中下一个单词是什么：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image026.png" alt="img"></p>
<p>​    困惑度是建立在信息论中关于交叉熵概念的基础上的，m对于p的交叉熵定义为：</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image028.png" alt="img"></p>

	  <div class="article-footer-copyright">

    本博客采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议(CC BY-NC-SA 4.0) 发布.</a>
</div>

	</div>

	
	<span id="/EskimoA000.github.io/2021/09/16/JMBook-Chapter-3-Reading/" class="leancloud-visitors view" data-flag-title="JMBook Chapter 3 Reading">
		<em class="post-meta-item-text"> Page View </em> <i class="leancloud-visitors-count"></i>
	</span>
	
	<div>
  	<center>

	<div class="pagination">

    
    
    <a type="button" class="btn btn-default disabled"><i class="fa fa-arrow-circle-o-left"></i>上一页</a>
    

    <a href="/EskimoA000.github.io/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/EskimoA000.github.io/2021/09/16/机器学习习题一/" type="button" class="btn btn-default ">下一页<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>


    </center>
	</div>
	
	<!-- comment -->
	<!--
<section id="comment">
    <h2 class="title">留言</h2>

    
</section>

-->
	
		<section id="comments" class="comments">
			<style>
			.comments{margin:30px;padding:10px;background:rgb(0, 0, 0)}
			@media screen and (max-width:800px){.comments{margin:auto;padding:10px;background:#000}}
			</style>
			<div id="vcomment" class="comment"></div>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script src="https://cdnjs.loli.net/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
<script>
var valineConfig = {"enable":true,"appId":"xx","appKey":"xx","placeholder":"提交评论时留下邮箱收到回复后将自动通知","visitor":true,"avatar":"monsterid","requiredFields":["nick","mail"]}
valineConfig.el='#vcomment';
new Valine(valineConfig);
    // new Valine({
    //     el: '#vcomment',
    //     appId: "",
    //     appKey: "",
    //     placeholder: "提交评论时留下邮箱收到回复后将自动通知",
    //     avatar:"monsterid",
    //     visitor: "true",
    //     requiredFields: "nick,mail".split(','),
    // });
</script>

		</section>
	
	</div> <!-- col-md-9/col-md-12 -->


	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2021-09-16 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/EskimoA000.github.io/categories/计算语言学/">计算语言学<span>1</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/EskimoA000.github.io/tags/N元语法/">N元语法<span>1</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

		

	</div>
	
		

</div><!-- row -->

<!--
 -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  
  &copy; 2021 张靖元's Blog
  
      powered by <a href="http://hexo.io/" target="_blank">Hexo</a>.Theme <a href="https://github.com/Ares-X/hexo-theme-freemind.bithack" target="_blank">freemind.bithack</a>  
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/EskimoA000.github.io/js/jquery.imagesloaded.min.js"></script>
<script src="/EskimoA000.github.io/js/gallery.js"></script>
<script src="/EskimoA000.github.io/js/bootstrap.min.js"></script>
<script src="/EskimoA000.github.io/js/main.js"></script>
<script src="/EskimoA000.github.io/js/search.js"></script> 


<link rel="stylesheet" href="/EskimoA000.github.io/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/EskimoA000.github.io/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/EskimoA000.github.io/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

<script src="/EskimoA000.github.io/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/EskimoA000.github.io/live2dw/assets/assets/wanko.model.json"},"display":{"position":"right","width":260,"height":600},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.8},"log":false});</script></body>
   </html>
