<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>test</title>
      <link href="/EskimoA000.github.io/2022/03/15/test/"/>
      <url>/EskimoA000.github.io/2022/03/15/test/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>1231231</p><p><img src="/EskimoA000.github.io/.top//blog\myblog\source_posts\test\ac.jpg" alt="123"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 高级机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ceshi</title>
      <link href="/EskimoA000.github.io/2022/03/15/ceshi/"/>
      <url>/EskimoA000.github.io/2022/03/15/ceshi/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>多模态信息处理-语言表征(词表征)</title>
      <link href="/EskimoA000.github.io/2021/10/22/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-%E8%AF%AD%E8%A8%80%E8%A1%A8%E5%BE%81-%E8%AF%8D%E8%A1%A8%E5%BE%81/"/>
      <url>/EskimoA000.github.io/2021/10/22/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-%E8%AF%AD%E8%A8%80%E8%A1%A8%E5%BE%81-%E8%AF%8D%E8%A1%A8%E5%BE%81/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>印欧语系是否需要分词？ 如果每一个词当作一个单词，没有联系（computer computers）   空格切分导致数据稀疏</p><p>英语 子词切分-将一个单词切分为若干连续的片段（bert 输入） 贪心算法</p><p>BPE（byte pair encoding）子词词表构建  初始为所有的字符‘a’‘b’…</p><p>编码 解码</p><h3 id="独热表征"><a href="#独热表征" class="headerlink" title="独热表征"></a>独热表征</h3><p>高维、稀疏、离散 所有向量都是正交的</p><p>优化：</p><p> 增加额外的特征：n v adj  加前后缀的特征</p><p>语义词典： WordNet、HowNet（中文 义原） 词的上位信息（飞行物 鸟），需要解决一词多义，词不全</p><p>词聚类特征：分布式上下文进行聚类</p><h3 id="分布式表征"><a href="#分布式表征" class="headerlink" title="分布式表征"></a>分布式表征</h3><p>词的含义可由上下文词的分布进行表示（奠基理论）</p><p>moon 上下文的词频（shinning trees bright dark）</p><p>降维到二维空间  语义相似度计算向量相似度来获得</p><p>文档分类 TF</p><p>降低高频词的权重  点互信息PMI</p><p>分布式表征进行降维 避免稀疏性，反映高阶共现关系</p><p>把一个矩阵分为三个矩阵相乘SVD （Singular Value Decomposition奇异值分解）  一旦训练完成，向量不能进行调整</p><p> 分布式表征缺点：训练速度慢   不能拓展到句子</p><h3 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h3><p>低维稠密的向量来表示词   但是用自监督的方法学习词向量</p><p>LDA：主题模型  </p><p>语言模型：估计若干个词的联合概率</p><p>N-gram 马尔可夫链</p><p>NNLM神经网络语言模型 前馈神经网络  通过查表获得词向量</p><p>SENNA 换词根据上下文 </p><p>Word2vec <em>过度简化</em> 是学习词嵌入表征的一种框架 收集大规模的文本语料  词表中的每一个词都被表示为一个向量</p><p>语料中文本每一个词组为中心词  几个词当一个窗口</p><p>Word2vec：CBOW模型  连续的词袋模型 根据一定窗口大小内的上下文，对当前时刻的词进行预测</p><p>本质上是分类问题 提升计算效率 1.分采样2.层次softmax</p><p>应用：词义相似度计算 词类比关系计算 知识图谱补全 推荐系统</p><p>skip-gram：根据目标词预测上下文中单词 输入层：one hot 权重wt</p><p>note2vec</p><p>Glove 结合word2vec和svd的思想 基于计数的思想 把———假设出来后去回归</p><h3 id="动态词嵌入"><a href="#动态词嵌入" class="headerlink" title="动态词嵌入"></a>动态词嵌入</h3><p>上面静态词嵌入是假设每个词只有一个词表示 无法处理一词多义的现象</p><p>ELMo （embeddings from language models）双向LSTM语言模型</p><p>用字符CNN表示词 一维的卷积 分别训练从右往左和从左往右</p><p>ELMo：词表征</p><p>上下文相关向量</p><p>RNN循环神经网络 构建<em>字符</em>级别的语言模型  核心是有<em>记忆</em>功能 隐藏层是以及单元  训练和测试（输出当输入）不太一样</p><p>时间反向传播算法 都计算完后  时间截断反向传播算法</p><p>Whh连乘用的是gating 梯度爆炸或者小时 （Resnet如何解决的）  </p><p>长短时记忆网络 ————没有连乘项</p>]]></content>
      
      
      <categories>
          
          <category> 多模态 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态信息处理-图像表征</title>
      <link href="/EskimoA000.github.io/2021/10/15/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-%E5%9B%BE%E5%83%8F%E8%A1%A8%E5%BE%81/"/>
      <url>/EskimoA000.github.io/2021/10/15/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-%E5%9B%BE%E5%83%8F%E8%A1%A8%E5%BE%81/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="图像表征-离散表征"><a href="#图像表征-离散表征" class="headerlink" title="图像表征-离散表征"></a>图像表征-离散表征</h2><p>dense向量表示的特征最多 先验就是生成</p><p>生成图像–离散特征最好</p><h4 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ-VAE"></a>VQ-VAE</h4><p>quantization量化</p><p>自编码器 输入层、表征层（限制器）、重构层（重构输入）</p><p>量化过程，把卷积层的特征 channel聚类  自编码器加聚类</p><p>编码器如何做下采样。。</p><p>embedding space（dictionary） 相当于字典  最近的找到index 再还原回来</p><p>copying gradients 梯度返回到最前面</p><p>最近邻检索 32float-&gt;32bit</p><p>解码器</p><p>上采样 从小特征到大特征 可以考虑反卷积（并不好）复制rest 32<em>32 -&gt;64</em> 64 再做输入输出大小相同的卷积</p><p>损失函数 </p><p>sg pytorch.detach</p><p>zq是词典里选的  交替更新</p><h4 id="VQ-VAE-2"><a href="#VQ-VAE-2" class="headerlink" title="VQ-VAE-2"></a>VQ-VAE-2</h4><p>多个不同分辨率图片的量化</p><h3 id="VQ-GAN"><a href="#VQ-GAN" class="headerlink" title="VQ-GAN"></a>VQ-GAN</h3><p>判断真假用判断器</p><p>图像风格迁移  风格像一张图 内容像另个一张图</p><p>感知loss 不在像素上算 在卷积层上向量计算 相减</p><p>GAN-生成对抗网络 输入分布、生成器（假钞厂）、判别器discriminator（验钞机） 从简单分布找到一个复杂分布生成数据分布，和目标数据分布尽可能接近</p><p>生成器、判断器交替训练  各自回合只更新自己的参数  </p><p>整体优化目标 先最大化判别器,再最小化生成器</p><p>期望积分</p><p>patch块</p>]]></content>
      
      
      <categories>
          
          <category> 多模态 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JMBook Chapter 7 Reading</title>
      <link href="/EskimoA000.github.io/2021/10/13/JMBook-Chapter-7-Reading/"/>
      <url>/EskimoA000.github.io/2021/10/13/JMBook-Chapter-7-Reading/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>JMBook Chapter 7 Reading-神经网络与神经语言模型</p><p>神经网络是语言处理的基本计算工具，“神经”起源于McCulloch-Pitts神经元，这是一种简化的人类神经元模型，是一种可以用命题逻辑描述的计算元素。</p><p>共享大部分相同的数学表达。但神经网络是一种比逻辑回归更强大的分类器，技术上具有单个“隐藏层”的最小的神经网络可以被用于学习任何函数；通过逻辑回归，通过基于领域知识开发了许多丰富的特征模板，将回归分类器应用于许多不同的任务，使用神经网络时，避免使用大量的手工提取功能，而是建立神经网络，将原始单词作为输入并学习推导特征作为学习分类过程的一部分；这章介绍作为分类器的前馈网络，并将其应用于语言建模的简单任务：为单词序列分配概率和预测即将出现的单词。</p><p>7.1 Units</p><p>一个神经网络块的构建是单个计算单元。 单元将实数值作为输入，对它们执行一些计算，并产生一个输出。i神经元对输入进行加权求和，额外项称为偏差项(Bias term):</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png" alt="https://pic4.zhimg.com/80/v2-ccefaa5185bc1acf24645e39497bc27b_720w.jpg"></p><p>用矢量（vector）符号表示这个加权和：将权重向量w，标量偏差b和输入向量x来讨论z，用点积替换总和:</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png" alt="https://pic4.zhimg.com/80/v2-ac4e33e1c427f9c6dbd1e2da29abf91f_720w.jpg"></p><p>神经单元将非线性函数f应用于z。将f的输出称为激活单元的激活值a:</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image006.png" alt="https://pic1.zhimg.com/80/v2-8c836f02e17da5707a08d44a378b27fc_720w.jpg"></p><p>其中有许多函数，例如Sigmoid、tanh、ReLu等。</p><p>7.2 The XOR problem</p><p>Minsky和Papert（1969）证明，单个神经单元无法计算其输入的一些非常简单的函数，这是多层网络需要的最聪明的证明之一。考虑计算两个输入，例如AND，OR，和XOR的基本逻辑函数的任务。</p><p>感知器（perceptron）是具有二进制输出且没有非线性激活功能的非常简单的神经单元。 感知器的输出y是0或1，并且如下计算（使用相同的权重w，输入x和偏差b）:</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image008.jpg" alt="https://pic4.zhimg.com/80/v2-2ca52c5a3f51da44ff6a5668d3f1019f_720w.jpg"></p><p>逻辑AND和OR函数的感知器为：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.jpg" alt="preview"></p><p>建立感知器来计算逻辑XOR是不可能的，这一重要结果依赖于理解感知器是线性分类器。XOR不是可线性分离（linearly separable）的可分离函数，但可以用神经网络来解决。</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image012.jpg" alt="preview"></p><p>使用两层基于ReLU的单元计算XOR，三个ReLU单元在两层: h1，h2（h为“隐藏层”）和y1。 箭头上的数字表示每个单位的权重w，并且我们将偏差b表示为+1的权重，偏差权重/单元为灰色。当x1=[0,0]时，隐藏层输出为[0,-1],将隐藏层变为Relu时输出为[0,0]。</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image014.png" alt="img"></p><p>7.3 Feed-Forward Neural Networks</p><p>​    前馈网络(feed-forward network)是一种多层网络，其中单元没有循环连接; 单位的输出每个层都传递给下一个更高层的单元，没有输出传递回更低层。</p><p>由于历史原因，多层网络，尤其是前馈网络，有时被称为多层感知器（multi-layer perceptron MLP）; 这是一个技术上的误称，因为现代多层网络中的单位不是感知器（感知器是纯线性的，但是现代网络由具有像sigmoids这样的非线性的单元组成），但是在某些时候这个名字被卡住了。 简单的前馈网络有三种节点：输入单元，隐藏单元和输出单元。</p><p>神经网络的核心是由隐藏单元组成的隐藏层（hidden layer），每个隐藏单元是神经单元，对其输入进行加权求和然后应用非线性。</p><p>在标准体系结构中，每个层都是完全连接(fully-connected)的，这意味着每个层中的每个单元将前一层中所有单元的输出作为输入，并且来自两个相邻层的每对单元之间存在链接。 因此，每个隐藏单元对所有输入单元求和。</p><p>z不能是分类器的输出，因为它是实数值的向量，而我们分类所需的是概率向量。 有一个方便的函数来归一化(normalizing)实数值的向量，将它转换为一个编码概率分布的向量（所有数字的softmax在0和1之间，总和为1）。</p><p>7.4 Training Neural Nets</p><p>前馈神经网络是监督机器学习的一个实例，其中我们知道每个x的正确输出y。</p><p>需要一个能够模拟系统输出和真实输出之间距离的损失函数（loss function），并且通常使用逻辑回归所用的损失，即交叉熵损失（cros-entropy）。使用梯度下降（gradien descent）优化算法找到最小化此损失函数的参数。梯度下降需要知道损失函数的梯度，包括关于每个参数的损失函数的偏导数。</p><p>对于逻辑回归，我们可以初始化梯度下降，所有权重和偏差值为0.在神经网络中，相反，我们需要用小的随机数初始化权重。 将输入值标准化（normalize）为0均值和单位方差也很有帮助。</p><p>各种形式的正则化用于防止过度拟合。dropout在训练期间随机丢弃一些单位及其与网络的连接。</p><p>超参数（Hyperparameter）调整也很重要。 神经网络的参数是权重W和偏差b; 这些都是通过梯度下降来学习的。 超参数是由算法设计者设置的，而不是以相同的方式学习，尽管它们必须进行调整。 超参数包括学习率h，小批量大小，模型架构（层数，每层隐藏节点数，激活函数的选择），如何正则化等等。 梯度下降本身也有许多结构变体，如Adam等。</p><p>7.5 Neural Language Models</p><p>语言模型：预测来自先前单词上下文的即将出现的单词。基于神经网络的语言模型比n-gram语言模型具有许多优点。其中神经语言模型不需要平滑，它们可以处理更长的背，并且它们可以概括相似单词的上下文。 对于给定大小的训练集，  神经语言模型比n-gram语言模型具有更高的预测准确性。此外，神经语言模型是我们为机器翻译，对话和语言生成等任务引入的许多模型的基础。另一方面，这种改进的性能有成本：神经网络语言模型比传统语言模型慢得多，因此对于许多任务来说，n-gram语言模型仍然是正确的工具。</p><p>前馈神经LM是标准前馈网络，在时间t作为输入，表示一些先前单词（wt-1; wt-2）的表示，并输出可能的下一单词的概率分布。就像n-gram LM一样，前馈神经LM通过基于N个先前的单词近似来近似给定整个先验上下P的单词的概率：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image016.jpg" alt="https://pic1.zhimg.com/80/v2-9af129aca5db2b53f5d5d45c078e46f4_720w.jpg"></p><p>7.5.1 Embeddings</p><p>在神经语言模型中，先前的上下文通过嵌入前一个单词来表示。 将先前的上下文表示为嵌入，而不是通过n-gram语言模型中使用的精确单词，允许神经语言模型比n-gram语言模型更好地推广到看不见的数据。</p><p>提出问题：</p><p>神经语言模型在预训练时嵌入效果好，还是在语言建模过程中从头开始学习嵌入好？</p>]]></content>
      
      
      <categories>
          
          <category> 计算语言学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与神经语言模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态信息处理-图像表征</title>
      <link href="/EskimoA000.github.io/2021/09/24/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-1/"/>
      <url>/EskimoA000.github.io/2021/09/24/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="L2-基于卷积神经网络的图像分类模型"><a href="#L2-基于卷积神经网络的图像分类模型" class="headerlink" title="L2 基于卷积神经网络的图像分类模型"></a>L2 基于卷积神经网络的图像分类模型</h1><h2 id="一-卷积层"><a href="#一-卷积层" class="headerlink" title="一.卷积层"></a>一.卷积层</h2><p>3通道RGB3<em>32</em>32 3072浮点型  全连接神经网络，加神经元 训练集提高，测试集下降  过拟合  原因是第一层时参数过多 1024*3072</p><p>初级视皮质 单个视觉细胞仅对部分区域的特定模式反应  </p><p>小区域检测模式，卷积神经网络</p><p>全图找特征，全连接神经网络  全连接层展开成一维</p><p>卷积的性质，（1）部分（鸟嘴）（2）相同的模式可能出现在图像不区域</p><p>卷积：局部特征提取器  对每个“子区域”进行加权求和 对每个“子区域”进行加权求和  卷积核：weight/filter/kernel</p><p>𝐖𝟏𝐱𝐇𝟏 —— 𝐅𝒘𝐱𝐅𝒉 ——𝐖𝟐𝐱𝐇𝟐  𝐖𝟐=𝐖𝟏−𝐅𝒘+𝟏  𝐇𝟐=𝐇𝟏−𝐅𝒉+𝟏</p><p>步幅Sw（stride）相邻的子区域很似，没有必要检测所有的子区域</p><p>𝐖𝟐=(𝐖𝟏−𝐅𝒘)/𝐒𝒘+𝟏   𝐇𝟐=(𝐇𝟏−𝐅𝒉)/𝐒𝒉+𝟏</p><p>填充Pw（padding）镜像填充等 填充0</p><p>𝐖𝟐=(𝐖𝟏+𝐏𝒘𝐱𝟐−𝐅𝒘)/𝐒𝒘+𝟏  𝐇𝟐=(𝐇𝟏+𝐏𝒉𝐱𝟐−𝐅𝒉)/𝐒𝒉+𝟏</p><p>输入通道 channel</p><p>输出通道  切面 每个通道代表一个特征</p><h2 id="二-汇聚层"><a href="#二-汇聚层" class="headerlink" title="二.汇聚层"></a>二.汇聚层</h2><p>降维 4<em>4变成2</em>2</p><p>pooling size（池化）+ stride</p><p>最大汇聚层（颜色）</p><p>平均汇聚层</p><h2 id="三-典型结构"><a href="#三-典型结构" class="headerlink" title="三.典型结构"></a>三.典型结构</h2><p>2012 AlexNet</p><p>2013 NIN 1×1卷积 调整通道数</p><p>2013 ZFNet</p><p>2014 VGGNet 3×3卷积  Block  更少的参数，更强的非线性</p><p>convolution </p><p>inception v4</p><p>2015 ResNet  残差模块 优化问题 非线性变化使得特征减少</p><h2 id="四-迁移网络"><a href="#四-迁移网络" class="headerlink" title="四.迁移网络"></a>四.迁移网络</h2><p>越往下学习速率越小 ImageNet</p><p>re’s’net 101 底层去掉</p>]]></content>
      
      
      <categories>
          
          <category> 多模态 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态信息处理</title>
      <link href="/EskimoA000.github.io/2021/09/22/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/"/>
      <url>/EskimoA000.github.io/2021/09/22/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a>思考：</h2><p>1.多模态表征能否在缩小多模态异质性上更近一步？</p><p>2.多模态关联能否发生在更加复杂的结构空间？</p><p>3.视觉信息的引入能否普遍帮助自然语言处理任务？</p>]]></content>
      
      
      <categories>
          
          <category> 多模态 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>wuti</title>
      <link href="/EskimoA000.github.io/2021/09/21/wuti/"/>
      <url>/EskimoA000.github.io/2021/09/21/wuti/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>但愿人长久，千里共婵娟。                                          ——2021中秋</p>]]></content>
      
      
      <categories>
          
          <category> 无题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JMBook Chapter 3 Reading</title>
      <link href="/EskimoA000.github.io/2021/09/16/JMBook-Chapter-3-Reading/"/>
      <url>/EskimoA000.github.io/2021/09/16/JMBook-Chapter-3-Reading/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Speech-and-Language-Processing-3rd-ed-draft"><a href="#Speech-and-Language-Processing-3rd-ed-draft" class="headerlink" title="Speech and Language Processing (3rd ed. draft)"></a><a href="http://www.web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing (3rd ed. draft)</a></h2><h2 id="Chapter-3-：N-gram-Language-Models"><a href="#Chapter-3-：N-gram-Language-Models" class="headerlink" title="Chapter 3 ：N-gram Language Models"></a><a href="http://www.web.stanford.edu/~jurafsky/slp3/3.pdf">Chapter 3 ：N-gram Language Models</a></h2><h2 id="章节内容综述（笔记）"><a href="#章节内容综述（笔记）" class="headerlink" title="章节内容综述（笔记）"></a>章节内容综述（笔记）</h2><p>N元语法模型（N-gram model）是一个概率模型，可以用来形式化地描述猜测单词的问题。比如要在噪声中或者歧义的输入中辨认出单词，N元语法就是必不可少的，例如应用到输入情况复杂的语音识别（speech recognition）中；同样在写作中拼写校正，语法校正和机器翻译都需要应用N元语法模型；N元语法对于辅助和替代性沟通系统同样重要。<br>   一个N元语法是包含N个单词的序列：二元语法是包含2个单词的序列（如please turn），三元语法是包含3个单词的序列（如 please turn your）。N元语法模型是根据前面出现的单词计算后一个单词的模型。</p><p>如果给定了某个单词w的历史h的条件下，我们用条件概率P来计算单词w的概率，来知道这个历史h后面单词w（例如the）的概率有多大。理想情况下，我们可以使用Web这样足够大的语料库，但是会出现三个问题：一是为了很好的估计出概率，我们经常觉得现有的Web规模还不够足够大；二是语言具有创造性，语言总是会不断创造出新的句子来；三是计算量太大，难以计算概率。</p><p>为此我们可以计算在一个句子序列中每一个单词都有一个特定的值的联合概率，记为               P(w1,w2,…,wn)，我们可以根据概率的链规则把这个概率分解：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png" alt="img"></p><p>我们可以通过把若干的条件概率相乘的办法来估计整个单词序列的联合概率，但是当前面的单词序列长度很大时，我们没有很好的办法来计算某个单词精确的条件概率。所以，我们想到一个处理方法：在计算某个单词的条件概率时，不考虑它之前全部的历史，而是只是考虑最接近该单词的若干个单词，从而近似地逼近该单词的历史。</p><p>例如，我们只用前面一个单词的条件概率P，来逼近后面给定的所有单词的概率，这就是二元语法模型，我们有以下的近似逼近公式:</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png" alt="img"></p><p>一个单词的概率只依赖它前面单词的概率的这种假设称为马尔可夫假设（Markov assumption）。马尔可夫模型是一种概率模型，我们没有必要查看很远的过去，就可以预见到某一个单位到来的概率。我们可以把二元语法模型推广到三元语法模型，在推广到N元语法模型（看过去的N-1个单词）。</p><p>所以在一个序列中，N元语法对于下一个单词的条件概率逼近的公式为：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image006.png" alt="img"></p><p>我们再将这概率逼近公式带入之前的概率链规则公式，得到：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image008.png" alt="img"></p><p>当我们估计二元语法或N元语法的概率时，我们可以用最简单和最直观的最大自然估计法（Maximum Likelihood Estimation，MLE）。我们可以把从语料库中得到的计数加以归一化（normalize），从而得到N元语法模型参数的MLE估计，进行归一化后，概率都处于0和1之间，以下公式为：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.png" alt="img"></p><p>其中C（xy）为计数函数，即单词x和单词y在一起出现的次数。</p><p>我们可以加以简化，因为以给定单词Wn-1开头的所有二元语法的计数必定等于单词Wn-1的一元语法的计数：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image012.png" alt="img"></p><p>附：用前面符号串的观察频率来除这个特定单词序列的观察频率，就得到N元语法概率的估计值，这个比值称为相对频率（relative frequency）。</p><p>N元语法评测：困惑度</p><p>评测语言模型性能的最好的方法是把这个语言模型嵌入到某种应用中去，并测试这种应用的总体性能，这种端到端的评测称为外在评测（extrinsic evaluation）。由于端对端的评测常常需要付出很高的代价，我们可以采用内在评测（intrinsic evaluation）度量，即与任何应用无关的模型质量的评测方法。</p><p>我们可以把在一些数据上训练，在另一些数据上测试的这种思想加以形式化，把它们分别称为训练集和测试集，或者训练语料库（training corpus）和测试语料库。我们把初始的测试集称为调试测试集，又称为开发集（development test set）。</p><p>对于统计模型与测试集匹配的情况，存在一个有用的度量方法，称为困惑度（perplexity）。困惑度（PP）是该语言模型指派给测试集的概率的函数。对于测试集W，困惑度就是用单词数归一化之后的测试集的概率：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image014.png" alt="img"></p><p>同样可以使用链规则来展开W的概率：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image016.png" alt="img"></p><p>另外一种研究困惑度的办法是语言的加权平均转移因子（weighted average branching factor），转移因子是指语言中的任何一个单词后面可能接续的单词的数量。</p><p>​    对那些小数据集的可变性而造成的糟糕的估计结果进行的修正，称为平滑（smoothing），我们将削减一些来自高计数的概率，用它们来填补那些零计数的概率，从而使得概率分布不至于太过于参差不一。</p><p>​    一个简单的平滑方法是：取二元语法的计数矩阵，在我们把它们归一化为概率之前，先给所有的计数加一。这种算法称为Laplace平滑(也被称为加一平滑)。Laplace平滑只是对于每一个计数加一，由于词汇表中有V个词，并且每一个词都有了增量，所以我们还有必要来调整分母，以便考虑到这个附加的观察值V：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image018.png" alt="img"></p><p>​    还有一种相关的方法是把平滑看成打折（discounting），也就是把某个非零的计数降下来，使得到的概率量可以指派给那些为零的计数。</p><p>​    我们可以通过分析得到加一平滑的折扣率太高了，一个很自然的想法是降低从高频词汇那里匀出来的概率，每个词的频数不是加1，而是加上一个小于1的浮点数k，这种方法被称为add-k smoothing：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image020.png" alt="img"></p><p>add-k平滑要求我们有选择k的算法，例如我们可以通过在dev集上优化k值。尽管add-k smoothing在一些任务上很有效（包括文本分类的任务），但它应用在语言模型上的效果仍然不是很好，平滑后的频数要么对原频数没什么改变，要么有着不恰当的折扣率。</p><p>dd-1平滑和add-k平滑可以解决N元模型的词汇零频数问题，但是N元模型的零值问题还包含另一种情况：假设我们要计算P(wn|wn-2wn-1)，但是我们没有wn-2wn-1wn的词组，我们可以退而求其次，计算二元概率P（wn|wn-1）代替三元概率。类似的，如果我们没有二元概率P（wn|wn-1），可以使用一元概率P(wn)进行代替。</p><p>有时在训练的时候少一点上下文可以模型的泛化能力。有两种方法利用这种n元模型之间的继承关系，分别是后退（backoff）和插值（interpolation）。当模型无法估计N元词组时，后退一步选用N-1元模型替代，这种方法称为后退。而插值的方法会利用所有的N元模型信息。</p><p>当估计一个三元词组的概率时，加权结合三元模型、二元模型和一元模型的结果。线性插值是相对简单的插值方法，将N个模型的结果做线性组合。利用线性插值估计三元词组的概率时，公式如下：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image022.png" alt="img"></p><p>其中所有的λ和为1，∑iλi=1</p><p>在使用后退算法计算概率时，如果N元词组计算的概率为0，使用N-1元近似来N元词组的概率。为了维持概率分布的正确性，在后退算法中，需要对higher-order概率折扣处理。如果higher-order的概率不进行折扣处理，而是直接使用lower-order的概率去近似，概率空间会被扩大，概率和将大于1。比如P（wn|wn-1）的概率明显比P(wn)概率小，如果直接用一元概率替代，所有二元概率的概率和将大于1。因此，我们需要用一个函数α来均衡概率分布。</p><p>这种有折扣的后退方法被称为Katz backoff。在Katz后退方法中，如果C(wnn-N+1)不为0，那么概率值为折扣概率P∗，如果C(wnn-N+1)是0，就递归地退回N-1的短上下文情境中计算概率并乘以α函数的结果，具体公式如下所示：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image024.png" alt="img"></p><p>在现代N元语法平滑中最普遍使用的一种方法是带插值的Kneser-Ney算法。这个算法的根源是一种称为绝对折扣（absolute discounting）的打折方法。</p><p>熵是信息的一种度量，可以用户来了解在一个特定的语法中的信息量是多少，度量给定语法和给定语言的匹配程度有多高，预测一个给定的N元语法中下一个单词是什么：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image026.png" alt="img"></p><p>​    困惑度是建立在信息论中关于交叉熵概念的基础上的，m对于p的交叉熵定义为：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image028.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> 计算语言学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> N元语法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习习题一</title>
      <link href="/EskimoA000.github.io/2021/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98%E4%B8%80/"/>
      <url>/EskimoA000.github.io/2021/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>Administra:</strong></p><p>•You need select at least two assignments from the list.</p><p>•Deliverables: In your writeup, include the best hyperparameters you used (e.g., training schedule, number of iterations, learning rate, backprop timesteps), your saved model parameters for your best model, and your evaluation result.</p><p><strong>Problem 1:</strong></p><p>•Due Date: <strong>Sep. 28, 2021</strong> (<strong>Tuesday</strong>)</p><p>•Task: handwritten digits recognition</p><p>•Data:  <a href="http://yann.lecun.com/exdb/mnist/">MNIST data set</a></p><p>•In this assignment you will practice putting together a simple image classification pipeline, based on the Softmax and the fully-connected classifier. The goals of this assignment are as follows:</p><p>– understand the basic <strong>Image Classification pipeline</strong> and the data-driven approach (train/predict stages)</p><p>– understand the train/val/test <strong>splits</strong> and the use of validation data for <strong>hyperparameter</strong> <strong>tuning</strong></p><p>– implement and apply a <strong>Softmax</strong> classifier</p><p>– implement and apply a <strong>Fully-connected neural network</strong> classifier</p><p>– understand the differences and tradeoffs between these classifiers</p><p>– implement various <strong>update rules</strong> used to optimize Neural Networks</p><h4 id><a href="#" class="headerlink" title></a></h4><p>•Do something extra! </p><p>​        – Maybe you can experiment with a different loss function and regularization? </p><p>​        – Or maybe you can experiment with different optimization algorithm (e.g., batch GD, online GD, mini-batch GD, SGD, or other optimization alg., e.g., Momentum, Adsgrad, Adam, Admax)</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
