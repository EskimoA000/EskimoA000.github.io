<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>读书杂谈-暗知识(持续更新)</title>
      <link href="/EskimoA000.github.io/2022/03/15/%E8%AF%BB%E4%B9%A6%E6%9D%82%E8%B0%88-%E6%9A%97%E7%9F%A5%E8%AF%86/"/>
      <url>/EskimoA000.github.io/2022/03/15/%E8%AF%BB%E4%B9%A6%E6%9D%82%E8%B0%88-%E6%9A%97%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>暗知识—你的认知正在阻碍你  [英]Michael Blastland著</p><p>伟大的意识形态是建立在关于基因与环境、先天与后天的相互矛盾的观念之上。</p><p>人们在面对“偶然性”时可能太过听天由命，同时在面对“噪音”时又太不屑一顾。</p><p>无须陷入虚无主义的绝望中</p><p>对进步的最大威胁，不是无知，而是对已有或未知知识的错觉。我们迫切需要摒弃一些错觉，这样才能将前路看的更加清晰。</p><p>犬儒主义：主张以追求普遍的善为人生之目的，为此必须抛弃一切物质享受和感官快乐。</p><p>人生需要回顾反思，但要想过得更好，我们更要积极向前看。</p><p>情景选择：若干不同挺像因素混杂在一起共同作用</p>]]></content>
      
      
      <categories>
          
          <category> 哲学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书杂谈 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cocktail</title>
      <link href="/EskimoA000.github.io/2022/03/15/cocktail/"/>
      <url>/EskimoA000.github.io/2022/03/15/cocktail/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Looking to Listen at the Cocktail Party:</p><p>A Speaker-Independent Audio-Visual Model for Speech Separation</p><p><em>前言</em></p><p>在嘈杂的环境中，人们非常善于把注意力集中在某个特定的人身上，在心理上“屏蔽”其他所有声音。这种能力被称为“鸡尾酒会效应”。</p><p> “语音分离”（Speech Separation）来自于“鸡尾酒会问题”，采集的音频信号中除了主讲话者之外，还有其他人说话声的干扰和噪音干扰。语音分离的目标就是从这些干扰中分离出主讲话者的语音。</p><p>根据干扰的不同，语音分离任务可以分为三类：</p><p>·         当干扰为噪声信号时，可以称为“语音增强”（Speech Enhancement）</p><p>·         当干扰为其他讲话者时，可以称为“多讲话者分离”（Speaker Separation）</p><p>·         当干扰为目标讲话者自己声音的反射波时，可以称为“解混响”（De-reverberation）</p><p>由于麦克风采集到的声音中可能包括噪声、其他人说话的声音、混响等干扰，不做语音分离、直接进行识别的话，会影响到识别的准确率。因此在语音识别的前端加上语音分离技术，把目标讲话者的声音和其它干扰分开就可以提高语音识别系统的鲁棒性，这从而也成为现代语音识别系统中不可或缺的一环。</p><p>基于深度学习的语音分离，主要是用基于深度学习的方法，从训练数据中学习语音、讲话者和噪音的特征，从而实现语音分离的目标。</p><p>l  这篇文章提出一个视觉-听觉联合模型，通过视觉信息来检测环境中谁在说什么并且分离出来；模型包含两个网络来分别分析视频和音频，通过融合层合并特征，最后使用传统的时频掩膜（Time-frequency masking）来分离语音部分；</p><p>l  训练过程中，搜集大量（90000）高质量、单说话人且头部位置比较正的视频，选取其中说话声音干净的部分，通过融合不同的视频或者给视频加噪声来创建训练集。</p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/1.jpg" alt="在这里插入图片描述"></p><p>一、论文主要工作</p><p>l  基于深度网络的联合视听模型(AVmodel)——结合视觉和听觉信号，将单个语音信号从其他讲话者和背景噪声等声音的混合中分离出来(<a href="http://looking-to-listen.github.io/">http://looking-to-listen.github.io/</a>)</p><p>l  使用视觉信息来提高源分离质量，并将分离的语音轨迹与视频中可见的讲话者相关联</p><p>l  提出了数据集AVSpeech，其是在YouTube收集了290,000个高质量的演讲，TED演讲和操作视频，然后从这些视频中自动提取大约4700个小时的视频片段(<a href="https://looking-to-listen.github.io/avspeech/index.html">https://looking-to-listen.github.io/avspeech/index.html</a>)</p><p>为了生成训练样本，论文首先从 YouTube 上收集 10 万个高质量讲座和演讲视频。然后从视频中提取带有清晰语音的片段（如没有音乐、观众声音或其他说话者声音的片段）和视频帧中只有一个说话者的片段。这样得到了数量可观的视频片段，镜头中出现的是单个人，且说话的时候没有背景干扰。之后，论文使用这些干净数据生成「合成鸡尾酒会」——将人脸视频、来自单独视频源的对应语音及从 AudioSet 获取的无语音背景噪声混合在一起。</p><p>使用这些数据，能够训练出基于多流卷积神经网络的模型，将合成鸡尾酒会片段分割成视频中每个说话者的单独音频流。网络输入是从每一帧检测到的说话者人脸缩略图中提取到的视觉特征，和视频声音的光谱图表征。训练过程中，网络（分别）学习视觉和听觉信号的编码，然后将其融合在一起形成一个联合音频-视觉表征。有了这种联合表征，网络可以学习为每个说话者输出时频掩码。输出掩码乘以带噪声的输入光谱图，然后被转换成时域波形，以获取每位说话者的单独、干净的语音信号，其中每个语音轨道来自视频中检测到的每一个人。</p><p>模型方法明显优于混合语音领域中当前最优的音频语音分割，并且模型独立于讲话者（训练一次，适用于任何讲话者），比最近的依赖于讲话者的视听语音分离方法产生更好的结果（需要为每个感兴趣的讲话者训练单独的模型）。</p><p>二、相关工作</p><p>Hershey et al. [2016]提出了一种称为深度聚类的方法，其中使用经过区别训练的语音嵌入来聚类和分离不同的来源。Hershey et al. [2016]也引入了无置换或置换不变损失函数的想法，但他们没有发现它运作良好。 Isik et al. [2016] and Yu et al. [2017]随后介绍了成功使用置换不变损失函数训练DNN的方法。</p><p>Hou J C, Wang S S, Lai Y H, et al. Audio-visual speech enhancement using multimodal deep convolutional neural networks。提出了一个基于多任务CNN的模型，功能：输出去噪语音频谱图以及输入口区域的重构。这些AV语音分离方法的主要局限性在于它们取决于说话者，这意味着必须分别为每个说话者训练专用的模型。</p><p>[Owens and Efros 2018]预测音频和视频流是否在时间上一致。从该自监督模型中提取的学习特征用于调节屏幕上/屏幕下讲话者源分离模型。Afouras等人[2018]通过使用网络预测去噪语音频谱图的幅度和相位来执行语音增强。Zhao等人[2018]和Gao等人[2018]提出了分离多个屏幕对象（如乐器）声音的密切相关问题。</p><p>…</p><p>三、数据集收集</p><p>Youtube上收集的数据集包含无干扰背景信号的语音片段。这些片段长度不等，长度在3到10秒之间，在每个片段中，视频中唯一可见的人脸和配乐中唯一可听到的声音都属于一个说话的人。总的来说，该数据集包含大约4700小时的视频片段，约有150000名不同的演讲者，涵盖各种各样的人、语言和面部姿势。</p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/2.png" alt="img"></p><p>首先收集了29万个高质量的在线公共演讲视频（a）从这些视频中，提取了清晰的语音片段（例如，没有混合音乐、观众声音或其他声源），并且在帧中可以看到讲话者。共有4700小时的视频剪辑，每个人在没有背景干扰的情况下讲话（b）。这些数据涵盖各种各样的人、语言和面部姿势，分布如（c）所示（使用自动分类器估计年龄和头部角度）。</p><p>之后就是用于创建数据集的视频和音频处理：</p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/3.png" alt="img"></p><p>（a）使用人脸检测和跟踪从视频中提取候选语音片段，并拒绝人脸模糊或不够正面的帧。（b）通过估计语音信噪比(SNR)来丢弃含噪语音的片段。该图旨在显示本论文采用的语音SNR估计器的准确性（以及数据集的质量）。在已知的信噪比水平下，比较了干净语音和非语音噪声的合成混合物的真实语音信噪比和预测的信噪比。预测的SNR值（以dB为单位）在每个SNR单元生成的60个混合值上求平均值，误差条表示1个标准。并丢弃预测的语音SNR低于17 dB（由图中的灰色虚线标记）的片段。</p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/5.png" alt="https://pic4.zhimg.com/80/v2-3a02cf6cda55925dd1d09d72cbff57af_720w.png"></p><p>Dataset creation pipeline：数据集收集过程有两个主要阶段：</p><p>\1. 使用Hoover等人[2017]的讲话者跟踪方法来检测一个人的视频片段，该人在说话时面部可见。从片段中丢弃模糊、照明不足或具有极端姿势的面部帧。如果某个片段的面帧丢失超过15%，则该片段将被完全丢弃。</p><p>\2. 细化语音片段，使其仅包含干净、无干扰的语音。使用预先训练的纯音频语音去噪网络，使用去噪后的输出作为干净信号的估计来预测给定片段的信噪比。该网络的体系结构与针对纯音频语音增强实施的体系结构相同，它是根据公共领域音频书籍的LibriVox集合中的语音进行训练的。拒绝估计信噪比低于阈值的段，实验发现阈值约为17 dB。</p><p>四、Audio-Visual Speech Separation model结构</p><p>输入：</p><p>\1. 视觉特征——给定一个包含多个演讲者的视频片段，本论文使用现成的面部检测器（如Google Cloud Vision API）在每个帧中查找面部75张。再使用FaceNet将人脸图像提取为一个Face embedding。对面部图像的原始像素进行了清晰化等实验，但这并没有提高性能。（每个讲话者共75张面部缩略图，假设以25 FPS播放3秒的片段）。</p><p>2.听觉特征——计算了3秒音频段的短时傅里叶变换（STFT），每个时频（TF）单元包含复数的实部和虚部，将两者都用作输入。执行幂律压缩（power-law），以防止强的音频压倒弱的音频。对噪声信号和纯净参考信号都进行相同的处理。在实验中，分离模型可以应用于任意长的视频片段。当在一帧中检测到多个说话的脸部时，模型可以接受多个脸部作为输入。</p><p>输出：</p><p>输出是一个乘法频谱mask，它描述了clean语音与背景噪音的<strong>时频关系</strong>。即每个输入演讲者的复数掩码（实数和虚数两个通道）。相应的频谱图是通过对有噪声的输入频谱图和输出掩码进行复数乘法来计算的。[Wang and Chen [2017], Wang [2014年]，观察到复数掩模比其他方法（例如，频谱图幅度的直接预测或时域波形的直接预测）的工作效果更好。其中用到了两个基于mask的方法：比率掩膜（RM）和复数比率掩膜（cRM）。</p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/6.png" alt="img"></p><p>Audio-Visual Speech Separation模型基于多流神经网络的架构：视频流以视频中每一帧检测到的人脸的缩略图作为输入，音频流以包含语音和背景噪声的视频音轨作为输入。</p><p>视觉流使用预训练的人脸识别模型提取每个缩略图的Face Embedding，然后使用空洞卷积神经网络学习视觉特征(6个卷积层)。</p><p>音频流首先计算输入信号的STFT以获得频谱图，然后使用类似的空洞卷积神经网络学习音频表示（15个卷积层）。</p><p>然后通过连接学习到的视觉和音频特征来创建联合的视听表示，随后使用Bi-LSTM和3个全连接层进行进一步处理。该网络为每个讲话者输出一个复频谱图掩码，该掩码乘以噪声输入，并转换回波形，以获得每个讲话者的隔离语音信号。</p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/7.png" alt="img"></p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/8.png" alt="img"></p><p>为了补偿音频和视频信号之间的采样率差异，论文对视频流的输出进行上采样，以匹配频谱图采样率（100Hz）。这是通过在每个视觉特征的时间维度中使用简单的最近邻插值来完成的。</p><p>AV融合：音频流和视频流通过连接每个流的特征映射进行组合，这些特征映射随后被馈送到Bi-LSTM中，接着是三个FC层。最终输出由每个输入讲话者的复掩模（两个通道，实通道和虚通道）组成。相应的谱图由带噪输入谱图和输出掩模的复数乘法计算。利用幂律压缩后的原始谱图和增强谱图之间的平方误差（L2）作为损失函数来训练网络。使用ISTFT获得最终输出波形。</p><p>模型支持视频中多个可视的讲话者音源隔离，每个讲话者音源由可视流表示，如图4所示。一个单独的，专用的模型是为每一个可视的讲话者训练的，例如，一个可视的讲话者有一个可视流的模型，两个讲话者的话就两个可视流的模型，等等。所有的可视流在卷积层上共享相同的权重。在这种情况下，在继续到Bi-LSTM之前，来自每个视频流的学习特征与学习的音频特征连接在一起。应该注意的是，在实践中，在讲话者数量未知或专用多讲话者模型不可用的一般情况下，可以使用以单个视觉流作为输入的模型。</p><p>具体细节：</p><p>l  implemented in TensorFlow</p><p>l  ReLU activations follow all network layers except for last (mask)</p><p>l  use a batch size of 6 samples and train with Adam optimizer for 5 million steps (batches) with a learning rate of 3 · 10−5 which is reduced by half every 1.8 million steps.</p><p>l  All audio is resampled to 16kHz, and stereo audio is converted to mono by taking only the left channel.</p><p>l  STFT is computed using a Hann window of length 25ms, hop length of 10ms, and FFT size of 512, resulting in an input audio feature of 257 × 298 × 2 scalars.</p><p>l  Power-law compression is performed with p = 0.3 (A0.3, where A is the input/output audio spectrogram).</p><p>l  the face embeddings from all videos to 25 framesper- second (FPS) before training and inference by either removing or replicating embeddings. This results in an input visual stream of 75 face embeddings.</p><p>五、论文结果</p><p>在与仅音频的比较：CHiME-2数据集上进行训练和评估时[Vincent et al. 2013年]被广泛用于语音增强工作，该论文的AO baseline实现了14.6 dB的信噪比，几乎与[2015 Erdogan et al]报告的最新单通道结果14.75 dB一样好。</p><p>在与最近的视听方法进行定量比较：</p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/9.png" alt="img"></p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/10.png" alt="img"></p><p>PESQ：PESQ分数范围从1（最差）到4.5（最好），3.8代表一般传统的电话的通话语音品质。</p><p>STOI：0-1范围，数值越大，可懂度更高。</p><p>SDR：信号失真比。</p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/11.png" alt="img"></p><p>六、论文总结</p><p>本论文提出了一种基于视听神经网络的单通道非特定人语音分离模型。本论文的模型在具有挑战性的场景中运行良好，包括多扬声器混合。为了训练这个模型，本论文创建了一个新的视听数据集，其中包含数千小时的视频片段，其中包含可见的扬声器和本论文从网络上收集的干净的语音。本论文展示了语音分离方面的最新成果，以及在视频字幕和语音识别方面的潜在应用。本论文还进行了大量实验来分析本论文的模型及其组件的行为。</p><p>AUDIO-VISUAL SPEECH ENHANCEMENT METHOD CONDITIONED ON THE LIP MOTION AND SPEAKER-DISCRIMINATIVE EMBEDDINGS</p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/12.png" alt="img"></p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/13.png" alt="img"></p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/14.png" alt="img"></p><p>AN EMPIRICAL STUDY OF VISUAL FEATURES FOR DNN BASED AUDIO-VISUAL SPEECH ENHANCEMENT IN MULTI-TALKER ENVIRONMENTS</p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/15.png" alt="img"></p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/16.png" alt="img"></p><p><img src="/EskimoA000.github.io/2022/03/15/cocktail/17.png" alt="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 鸡尾酒会 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图片功能TEST</title>
      <link href="/EskimoA000.github.io/2022/03/15/test/"/>
      <url>/EskimoA000.github.io/2022/03/15/test/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="/EskimoA000.github.io/2022/03/15/test/ac.jpg"></p>]]></content>
      
      
      
        <tags>
            
            <tag> TEST </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态信息处理-语言表征(词表征)</title>
      <link href="/EskimoA000.github.io/2021/10/22/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-%E8%AF%AD%E8%A8%80%E8%A1%A8%E5%BE%81-%E8%AF%8D%E8%A1%A8%E5%BE%81/"/>
      <url>/EskimoA000.github.io/2021/10/22/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-%E8%AF%AD%E8%A8%80%E8%A1%A8%E5%BE%81-%E8%AF%8D%E8%A1%A8%E5%BE%81/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>印欧语系是否需要分词？ 如果每一个词当作一个单词，没有联系（computer computers）   空格切分导致数据稀疏</p><p>英语 子词切分-将一个单词切分为若干连续的片段（bert 输入） 贪心算法</p><p>BPE（byte pair encoding）子词词表构建  初始为所有的字符‘a’‘b’…</p><p>编码 解码</p><h3 id="独热表征"><a href="#独热表征" class="headerlink" title="独热表征"></a>独热表征</h3><p>高维、稀疏、离散 所有向量都是正交的</p><p>优化：</p><p> 增加额外的特征：n v adj  加前后缀的特征</p><p>语义词典： WordNet、HowNet（中文 义原） 词的上位信息（飞行物 鸟），需要解决一词多义，词不全</p><p>词聚类特征：分布式上下文进行聚类</p><h3 id="分布式表征"><a href="#分布式表征" class="headerlink" title="分布式表征"></a>分布式表征</h3><p>词的含义可由上下文词的分布进行表示（奠基理论）</p><p>moon 上下文的词频（shinning trees bright dark）</p><p>降维到二维空间  语义相似度计算向量相似度来获得</p><p>文档分类 TF</p><p>降低高频词的权重  点互信息PMI</p><p>分布式表征进行降维 避免稀疏性，反映高阶共现关系</p><p>把一个矩阵分为三个矩阵相乘SVD （Singular Value Decomposition奇异值分解）  一旦训练完成，向量不能进行调整</p><p> 分布式表征缺点：训练速度慢   不能拓展到句子</p><h3 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h3><p>低维稠密的向量来表示词   但是用自监督的方法学习词向量</p><p>LDA：主题模型  </p><p>语言模型：估计若干个词的联合概率</p><p>N-gram 马尔可夫链</p><p>NNLM神经网络语言模型 前馈神经网络  通过查表获得词向量</p><p>SENNA 换词根据上下文 </p><p>Word2vec <em>过度简化</em> 是学习词嵌入表征的一种框架 收集大规模的文本语料  词表中的每一个词都被表示为一个向量</p><p>语料中文本每一个词组为中心词  几个词当一个窗口</p><p>Word2vec：CBOW模型  连续的词袋模型 根据一定窗口大小内的上下文，对当前时刻的词进行预测</p><p>本质上是分类问题 提升计算效率 1.分采样2.层次softmax</p><p>应用：词义相似度计算 词类比关系计算 知识图谱补全 推荐系统</p><p>skip-gram：根据目标词预测上下文中单词 输入层：one hot 权重wt</p><p>note2vec</p><p>Glove 结合word2vec和svd的思想 基于计数的思想 把———假设出来后去回归</p><h3 id="动态词嵌入"><a href="#动态词嵌入" class="headerlink" title="动态词嵌入"></a>动态词嵌入</h3><p>上面静态词嵌入是假设每个词只有一个词表示 无法处理一词多义的现象</p><p>ELMo （embeddings from language models）双向LSTM语言模型</p><p>用字符CNN表示词 一维的卷积 分别训练从右往左和从左往右</p><p>ELMo：词表征</p><p>上下文相关向量</p><p>RNN循环神经网络 构建<em>字符</em>级别的语言模型  核心是有<em>记忆</em>功能 隐藏层是以及单元  训练和测试（输出当输入）不太一样</p><p>时间反向传播算法 都计算完后  时间截断反向传播算法</p><p>Whh连乘用的是gating 梯度爆炸或者小时 （Resnet如何解决的）  </p><p>长短时记忆网络 ————没有连乘项</p>]]></content>
      
      
      <categories>
          
          <category> 多模态 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态信息处理-图像表征</title>
      <link href="/EskimoA000.github.io/2021/10/15/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-%E5%9B%BE%E5%83%8F%E8%A1%A8%E5%BE%81/"/>
      <url>/EskimoA000.github.io/2021/10/15/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-%E5%9B%BE%E5%83%8F%E8%A1%A8%E5%BE%81/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="图像表征-离散表征"><a href="#图像表征-离散表征" class="headerlink" title="图像表征-离散表征"></a>图像表征-离散表征</h2><p>dense向量表示的特征最多 先验就是生成</p><p>生成图像–离散特征最好</p><h4 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ-VAE"></a>VQ-VAE</h4><p>quantization量化</p><p>自编码器 输入层、表征层（限制器）、重构层（重构输入）</p><p>量化过程，把卷积层的特征 channel聚类  自编码器加聚类</p><p>编码器如何做下采样。。</p><p>embedding space（dictionary） 相当于字典  最近的找到index 再还原回来</p><p>copying gradients 梯度返回到最前面</p><p>最近邻检索 32float-&gt;32bit</p><p>解码器</p><p>上采样 从小特征到大特征 可以考虑反卷积（并不好）复制rest 32<em>32 -&gt;64</em> 64 再做输入输出大小相同的卷积</p><p>损失函数 </p><p>sg pytorch.detach</p><p>zq是词典里选的  交替更新</p><h4 id="VQ-VAE-2"><a href="#VQ-VAE-2" class="headerlink" title="VQ-VAE-2"></a>VQ-VAE-2</h4><p>多个不同分辨率图片的量化</p><h3 id="VQ-GAN"><a href="#VQ-GAN" class="headerlink" title="VQ-GAN"></a>VQ-GAN</h3><p>判断真假用判断器</p><p>图像风格迁移  风格像一张图 内容像另个一张图</p><p>感知loss 不在像素上算 在卷积层上向量计算 相减</p><p>GAN-生成对抗网络 输入分布、生成器（假钞厂）、判别器discriminator（验钞机） 从简单分布找到一个复杂分布生成数据分布，和目标数据分布尽可能接近</p><p>生成器、判断器交替训练  各自回合只更新自己的参数  </p><p>整体优化目标 先最大化判别器,再最小化生成器</p><p>期望积分</p><p>patch块</p>]]></content>
      
      
      <categories>
          
          <category> 多模态 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JMBook Chapter 7 Reading</title>
      <link href="/EskimoA000.github.io/2021/10/13/JMBook-Chapter-7-Reading/"/>
      <url>/EskimoA000.github.io/2021/10/13/JMBook-Chapter-7-Reading/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>JMBook Chapter 7 Reading-神经网络与神经语言模型</p><p>神经网络是语言处理的基本计算工具，“神经”起源于McCulloch-Pitts神经元，这是一种简化的人类神经元模型，是一种可以用命题逻辑描述的计算元素。</p><p>共享大部分相同的数学表达。但神经网络是一种比逻辑回归更强大的分类器，技术上具有单个“隐藏层”的最小的神经网络可以被用于学习任何函数；通过逻辑回归，通过基于领域知识开发了许多丰富的特征模板，将回归分类器应用于许多不同的任务，使用神经网络时，避免使用大量的手工提取功能，而是建立神经网络，将原始单词作为输入并学习推导特征作为学习分类过程的一部分；这章介绍作为分类器的前馈网络，并将其应用于语言建模的简单任务：为单词序列分配概率和预测即将出现的单词。</p><p>7.1 Units</p><p>一个神经网络块的构建是单个计算单元。 单元将实数值作为输入，对它们执行一些计算，并产生一个输出。i神经元对输入进行加权求和，额外项称为偏差项(Bias term):</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png" alt="https://pic4.zhimg.com/80/v2-ccefaa5185bc1acf24645e39497bc27b_720w.jpg"></p><p>用矢量（vector）符号表示这个加权和：将权重向量w，标量偏差b和输入向量x来讨论z，用点积替换总和:</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png" alt="https://pic4.zhimg.com/80/v2-ac4e33e1c427f9c6dbd1e2da29abf91f_720w.jpg"></p><p>神经单元将非线性函数f应用于z。将f的输出称为激活单元的激活值a:</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image006.png" alt="https://pic1.zhimg.com/80/v2-8c836f02e17da5707a08d44a378b27fc_720w.jpg"></p><p>其中有许多函数，例如Sigmoid、tanh、ReLu等。</p><p>7.2 The XOR problem</p><p>Minsky和Papert（1969）证明，单个神经单元无法计算其输入的一些非常简单的函数，这是多层网络需要的最聪明的证明之一。考虑计算两个输入，例如AND，OR，和XOR的基本逻辑函数的任务。</p><p>感知器（perceptron）是具有二进制输出且没有非线性激活功能的非常简单的神经单元。 感知器的输出y是0或1，并且如下计算（使用相同的权重w，输入x和偏差b）:</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image008.jpg" alt="https://pic4.zhimg.com/80/v2-2ca52c5a3f51da44ff6a5668d3f1019f_720w.jpg"></p><p>逻辑AND和OR函数的感知器为：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.jpg" alt="preview"></p><p>建立感知器来计算逻辑XOR是不可能的，这一重要结果依赖于理解感知器是线性分类器。XOR不是可线性分离（linearly separable）的可分离函数，但可以用神经网络来解决。</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image012.jpg" alt="preview"></p><p>使用两层基于ReLU的单元计算XOR，三个ReLU单元在两层: h1，h2（h为“隐藏层”）和y1。 箭头上的数字表示每个单位的权重w，并且我们将偏差b表示为+1的权重，偏差权重/单元为灰色。当x1=[0,0]时，隐藏层输出为[0,-1],将隐藏层变为Relu时输出为[0,0]。</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image014.png" alt="img"></p><p>7.3 Feed-Forward Neural Networks</p><p>​    前馈网络(feed-forward network)是一种多层网络，其中单元没有循环连接; 单位的输出每个层都传递给下一个更高层的单元，没有输出传递回更低层。</p><p>由于历史原因，多层网络，尤其是前馈网络，有时被称为多层感知器（multi-layer perceptron MLP）; 这是一个技术上的误称，因为现代多层网络中的单位不是感知器（感知器是纯线性的，但是现代网络由具有像sigmoids这样的非线性的单元组成），但是在某些时候这个名字被卡住了。 简单的前馈网络有三种节点：输入单元，隐藏单元和输出单元。</p><p>神经网络的核心是由隐藏单元组成的隐藏层（hidden layer），每个隐藏单元是神经单元，对其输入进行加权求和然后应用非线性。</p><p>在标准体系结构中，每个层都是完全连接(fully-connected)的，这意味着每个层中的每个单元将前一层中所有单元的输出作为输入，并且来自两个相邻层的每对单元之间存在链接。 因此，每个隐藏单元对所有输入单元求和。</p><p>z不能是分类器的输出，因为它是实数值的向量，而我们分类所需的是概率向量。 有一个方便的函数来归一化(normalizing)实数值的向量，将它转换为一个编码概率分布的向量（所有数字的softmax在0和1之间，总和为1）。</p><p>7.4 Training Neural Nets</p><p>前馈神经网络是监督机器学习的一个实例，其中我们知道每个x的正确输出y。</p><p>需要一个能够模拟系统输出和真实输出之间距离的损失函数（loss function），并且通常使用逻辑回归所用的损失，即交叉熵损失（cros-entropy）。使用梯度下降（gradien descent）优化算法找到最小化此损失函数的参数。梯度下降需要知道损失函数的梯度，包括关于每个参数的损失函数的偏导数。</p><p>对于逻辑回归，我们可以初始化梯度下降，所有权重和偏差值为0.在神经网络中，相反，我们需要用小的随机数初始化权重。 将输入值标准化（normalize）为0均值和单位方差也很有帮助。</p><p>各种形式的正则化用于防止过度拟合。dropout在训练期间随机丢弃一些单位及其与网络的连接。</p><p>超参数（Hyperparameter）调整也很重要。 神经网络的参数是权重W和偏差b; 这些都是通过梯度下降来学习的。 超参数是由算法设计者设置的，而不是以相同的方式学习，尽管它们必须进行调整。 超参数包括学习率h，小批量大小，模型架构（层数，每层隐藏节点数，激活函数的选择），如何正则化等等。 梯度下降本身也有许多结构变体，如Adam等。</p><p>7.5 Neural Language Models</p><p>语言模型：预测来自先前单词上下文的即将出现的单词。基于神经网络的语言模型比n-gram语言模型具有许多优点。其中神经语言模型不需要平滑，它们可以处理更长的背，并且它们可以概括相似单词的上下文。 对于给定大小的训练集，  神经语言模型比n-gram语言模型具有更高的预测准确性。此外，神经语言模型是我们为机器翻译，对话和语言生成等任务引入的许多模型的基础。另一方面，这种改进的性能有成本：神经网络语言模型比传统语言模型慢得多，因此对于许多任务来说，n-gram语言模型仍然是正确的工具。</p><p>前馈神经LM是标准前馈网络，在时间t作为输入，表示一些先前单词（wt-1; wt-2）的表示，并输出可能的下一单词的概率分布。就像n-gram LM一样，前馈神经LM通过基于N个先前的单词近似来近似给定整个先验上下P的单词的概率：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image016.jpg" alt="https://pic1.zhimg.com/80/v2-9af129aca5db2b53f5d5d45c078e46f4_720w.jpg"></p><p>7.5.1 Embeddings</p><p>在神经语言模型中，先前的上下文通过嵌入前一个单词来表示。 将先前的上下文表示为嵌入，而不是通过n-gram语言模型中使用的精确单词，允许神经语言模型比n-gram语言模型更好地推广到看不见的数据。</p><p>提出问题：</p><p>神经语言模型在预训练时嵌入效果好，还是在语言建模过程中从头开始学习嵌入好？</p>]]></content>
      
      
      <categories>
          
          <category> 计算语言学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与神经语言模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态信息处理-图像表征</title>
      <link href="/EskimoA000.github.io/2021/09/24/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-1/"/>
      <url>/EskimoA000.github.io/2021/09/24/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86-1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="L2-基于卷积神经网络的图像分类模型"><a href="#L2-基于卷积神经网络的图像分类模型" class="headerlink" title="L2 基于卷积神经网络的图像分类模型"></a>L2 基于卷积神经网络的图像分类模型</h1><h2 id="一-卷积层"><a href="#一-卷积层" class="headerlink" title="一.卷积层"></a>一.卷积层</h2><p>3通道RGB3<em>32</em>32 3072浮点型  全连接神经网络，加神经元 训练集提高，测试集下降  过拟合  原因是第一层时参数过多 1024*3072</p><p>初级视皮质 单个视觉细胞仅对部分区域的特定模式反应  </p><p>小区域检测模式，卷积神经网络</p><p>全图找特征，全连接神经网络  全连接层展开成一维</p><p>卷积的性质，（1）部分（鸟嘴）（2）相同的模式可能出现在图像不区域</p><p>卷积：局部特征提取器  对每个“子区域”进行加权求和 对每个“子区域”进行加权求和  卷积核：weight/filter/kernel</p><p>𝐖𝟏𝐱𝐇𝟏 —— 𝐅𝒘𝐱𝐅𝒉 ——𝐖𝟐𝐱𝐇𝟐  𝐖𝟐=𝐖𝟏−𝐅𝒘+𝟏  𝐇𝟐=𝐇𝟏−𝐅𝒉+𝟏</p><p>步幅Sw（stride）相邻的子区域很似，没有必要检测所有的子区域</p><p>𝐖𝟐=(𝐖𝟏−𝐅𝒘)/𝐒𝒘+𝟏   𝐇𝟐=(𝐇𝟏−𝐅𝒉)/𝐒𝒉+𝟏</p><p>填充Pw（padding）镜像填充等 填充0</p><p>𝐖𝟐=(𝐖𝟏+𝐏𝒘𝐱𝟐−𝐅𝒘)/𝐒𝒘+𝟏  𝐇𝟐=(𝐇𝟏+𝐏𝒉𝐱𝟐−𝐅𝒉)/𝐒𝒉+𝟏</p><p>输入通道 channel</p><p>输出通道  切面 每个通道代表一个特征</p><h2 id="二-汇聚层"><a href="#二-汇聚层" class="headerlink" title="二.汇聚层"></a>二.汇聚层</h2><p>降维 4<em>4变成2</em>2</p><p>pooling size（池化）+ stride</p><p>最大汇聚层（颜色）</p><p>平均汇聚层</p><h2 id="三-典型结构"><a href="#三-典型结构" class="headerlink" title="三.典型结构"></a>三.典型结构</h2><p>2012 AlexNet</p><p>2013 NIN 1×1卷积 调整通道数</p><p>2013 ZFNet</p><p>2014 VGGNet 3×3卷积  Block  更少的参数，更强的非线性</p><p>convolution </p><p>inception v4</p><p>2015 ResNet  残差模块 优化问题 非线性变化使得特征减少</p><h2 id="四-迁移网络"><a href="#四-迁移网络" class="headerlink" title="四.迁移网络"></a>四.迁移网络</h2><p>越往下学习速率越小 ImageNet</p><p>re’s’net 101 底层去掉</p>]]></content>
      
      
      <categories>
          
          <category> 多模态 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态信息处理</title>
      <link href="/EskimoA000.github.io/2021/09/22/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/"/>
      <url>/EskimoA000.github.io/2021/09/22/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a>思考：</h2><p>1.多模态表征能否在缩小多模态异质性上更近一步？</p><p>2.多模态关联能否发生在更加复杂的结构空间？</p><p>3.视觉信息的引入能否普遍帮助自然语言处理任务？</p>]]></content>
      
      
      <categories>
          
          <category> 多模态 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>wuti</title>
      <link href="/EskimoA000.github.io/2021/09/21/wuti/"/>
      <url>/EskimoA000.github.io/2021/09/21/wuti/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>但愿人长久，千里共婵娟。                                          ——2021中秋</p>]]></content>
      
      
      <categories>
          
          <category> 无题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JMBook Chapter 3 Reading</title>
      <link href="/EskimoA000.github.io/2021/09/16/JMBook-Chapter-3-Reading/"/>
      <url>/EskimoA000.github.io/2021/09/16/JMBook-Chapter-3-Reading/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Speech-and-Language-Processing-3rd-ed-draft"><a href="#Speech-and-Language-Processing-3rd-ed-draft" class="headerlink" title="Speech and Language Processing (3rd ed. draft)"></a><a href="http://www.web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing (3rd ed. draft)</a></h2><h2 id="Chapter-3-：N-gram-Language-Models"><a href="#Chapter-3-：N-gram-Language-Models" class="headerlink" title="Chapter 3 ：N-gram Language Models"></a><a href="http://www.web.stanford.edu/~jurafsky/slp3/3.pdf">Chapter 3 ：N-gram Language Models</a></h2><h2 id="章节内容综述（笔记）"><a href="#章节内容综述（笔记）" class="headerlink" title="章节内容综述（笔记）"></a>章节内容综述（笔记）</h2><p>N元语法模型（N-gram model）是一个概率模型，可以用来形式化地描述猜测单词的问题。比如要在噪声中或者歧义的输入中辨认出单词，N元语法就是必不可少的，例如应用到输入情况复杂的语音识别（speech recognition）中；同样在写作中拼写校正，语法校正和机器翻译都需要应用N元语法模型；N元语法对于辅助和替代性沟通系统同样重要。<br>   一个N元语法是包含N个单词的序列：二元语法是包含2个单词的序列（如please turn），三元语法是包含3个单词的序列（如 please turn your）。N元语法模型是根据前面出现的单词计算后一个单词的模型。</p><p>如果给定了某个单词w的历史h的条件下，我们用条件概率P来计算单词w的概率，来知道这个历史h后面单词w（例如the）的概率有多大。理想情况下，我们可以使用Web这样足够大的语料库，但是会出现三个问题：一是为了很好的估计出概率，我们经常觉得现有的Web规模还不够足够大；二是语言具有创造性，语言总是会不断创造出新的句子来；三是计算量太大，难以计算概率。</p><p>为此我们可以计算在一个句子序列中每一个单词都有一个特定的值的联合概率，记为               P(w1,w2,…,wn)，我们可以根据概率的链规则把这个概率分解：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png" alt="img"></p><p>我们可以通过把若干的条件概率相乘的办法来估计整个单词序列的联合概率，但是当前面的单词序列长度很大时，我们没有很好的办法来计算某个单词精确的条件概率。所以，我们想到一个处理方法：在计算某个单词的条件概率时，不考虑它之前全部的历史，而是只是考虑最接近该单词的若干个单词，从而近似地逼近该单词的历史。</p><p>例如，我们只用前面一个单词的条件概率P，来逼近后面给定的所有单词的概率，这就是二元语法模型，我们有以下的近似逼近公式:</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png" alt="img"></p><p>一个单词的概率只依赖它前面单词的概率的这种假设称为马尔可夫假设（Markov assumption）。马尔可夫模型是一种概率模型，我们没有必要查看很远的过去，就可以预见到某一个单位到来的概率。我们可以把二元语法模型推广到三元语法模型，在推广到N元语法模型（看过去的N-1个单词）。</p><p>所以在一个序列中，N元语法对于下一个单词的条件概率逼近的公式为：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image006.png" alt="img"></p><p>我们再将这概率逼近公式带入之前的概率链规则公式，得到：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image008.png" alt="img"></p><p>当我们估计二元语法或N元语法的概率时，我们可以用最简单和最直观的最大自然估计法（Maximum Likelihood Estimation，MLE）。我们可以把从语料库中得到的计数加以归一化（normalize），从而得到N元语法模型参数的MLE估计，进行归一化后，概率都处于0和1之间，以下公式为：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.png" alt="img"></p><p>其中C（xy）为计数函数，即单词x和单词y在一起出现的次数。</p><p>我们可以加以简化，因为以给定单词Wn-1开头的所有二元语法的计数必定等于单词Wn-1的一元语法的计数：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image012.png" alt="img"></p><p>附：用前面符号串的观察频率来除这个特定单词序列的观察频率，就得到N元语法概率的估计值，这个比值称为相对频率（relative frequency）。</p><p>N元语法评测：困惑度</p><p>评测语言模型性能的最好的方法是把这个语言模型嵌入到某种应用中去，并测试这种应用的总体性能，这种端到端的评测称为外在评测（extrinsic evaluation）。由于端对端的评测常常需要付出很高的代价，我们可以采用内在评测（intrinsic evaluation）度量，即与任何应用无关的模型质量的评测方法。</p><p>我们可以把在一些数据上训练，在另一些数据上测试的这种思想加以形式化，把它们分别称为训练集和测试集，或者训练语料库（training corpus）和测试语料库。我们把初始的测试集称为调试测试集，又称为开发集（development test set）。</p><p>对于统计模型与测试集匹配的情况，存在一个有用的度量方法，称为困惑度（perplexity）。困惑度（PP）是该语言模型指派给测试集的概率的函数。对于测试集W，困惑度就是用单词数归一化之后的测试集的概率：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image014.png" alt="img"></p><p>同样可以使用链规则来展开W的概率：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image016.png" alt="img"></p><p>另外一种研究困惑度的办法是语言的加权平均转移因子（weighted average branching factor），转移因子是指语言中的任何一个单词后面可能接续的单词的数量。</p><p>​    对那些小数据集的可变性而造成的糟糕的估计结果进行的修正，称为平滑（smoothing），我们将削减一些来自高计数的概率，用它们来填补那些零计数的概率，从而使得概率分布不至于太过于参差不一。</p><p>​    一个简单的平滑方法是：取二元语法的计数矩阵，在我们把它们归一化为概率之前，先给所有的计数加一。这种算法称为Laplace平滑(也被称为加一平滑)。Laplace平滑只是对于每一个计数加一，由于词汇表中有V个词，并且每一个词都有了增量，所以我们还有必要来调整分母，以便考虑到这个附加的观察值V：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image018.png" alt="img"></p><p>​    还有一种相关的方法是把平滑看成打折（discounting），也就是把某个非零的计数降下来，使得到的概率量可以指派给那些为零的计数。</p><p>​    我们可以通过分析得到加一平滑的折扣率太高了，一个很自然的想法是降低从高频词汇那里匀出来的概率，每个词的频数不是加1，而是加上一个小于1的浮点数k，这种方法被称为add-k smoothing：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image020.png" alt="img"></p><p>add-k平滑要求我们有选择k的算法，例如我们可以通过在dev集上优化k值。尽管add-k smoothing在一些任务上很有效（包括文本分类的任务），但它应用在语言模型上的效果仍然不是很好，平滑后的频数要么对原频数没什么改变，要么有着不恰当的折扣率。</p><p>dd-1平滑和add-k平滑可以解决N元模型的词汇零频数问题，但是N元模型的零值问题还包含另一种情况：假设我们要计算P(wn|wn-2wn-1)，但是我们没有wn-2wn-1wn的词组，我们可以退而求其次，计算二元概率P（wn|wn-1）代替三元概率。类似的，如果我们没有二元概率P（wn|wn-1），可以使用一元概率P(wn)进行代替。</p><p>有时在训练的时候少一点上下文可以模型的泛化能力。有两种方法利用这种n元模型之间的继承关系，分别是后退（backoff）和插值（interpolation）。当模型无法估计N元词组时，后退一步选用N-1元模型替代，这种方法称为后退。而插值的方法会利用所有的N元模型信息。</p><p>当估计一个三元词组的概率时，加权结合三元模型、二元模型和一元模型的结果。线性插值是相对简单的插值方法，将N个模型的结果做线性组合。利用线性插值估计三元词组的概率时，公式如下：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image022.png" alt="img"></p><p>其中所有的λ和为1，∑iλi=1</p><p>在使用后退算法计算概率时，如果N元词组计算的概率为0，使用N-1元近似来N元词组的概率。为了维持概率分布的正确性，在后退算法中，需要对higher-order概率折扣处理。如果higher-order的概率不进行折扣处理，而是直接使用lower-order的概率去近似，概率空间会被扩大，概率和将大于1。比如P（wn|wn-1）的概率明显比P(wn)概率小，如果直接用一元概率替代，所有二元概率的概率和将大于1。因此，我们需要用一个函数α来均衡概率分布。</p><p>这种有折扣的后退方法被称为Katz backoff。在Katz后退方法中，如果C(wnn-N+1)不为0，那么概率值为折扣概率P∗，如果C(wnn-N+1)是0，就递归地退回N-1的短上下文情境中计算概率并乘以α函数的结果，具体公式如下所示：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image024.png" alt="img"></p><p>在现代N元语法平滑中最普遍使用的一种方法是带插值的Kneser-Ney算法。这个算法的根源是一种称为绝对折扣（absolute discounting）的打折方法。</p><p>熵是信息的一种度量，可以用户来了解在一个特定的语法中的信息量是多少，度量给定语法和给定语言的匹配程度有多高，预测一个给定的N元语法中下一个单词是什么：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image026.png" alt="img"></p><p>​    困惑度是建立在信息论中关于交叉熵概念的基础上的，m对于p的交叉熵定义为：</p><p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image028.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> 计算语言学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> N元语法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习习题一</title>
      <link href="/EskimoA000.github.io/2021/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98%E4%B8%80/"/>
      <url>/EskimoA000.github.io/2021/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%A0%E9%A2%98%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\EskimoA000.github.io\assets\css\APlayer.min.css"><script src="\EskimoA000.github.io\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>Administra:</strong></p><p>•You need select at least two assignments from the list.</p><p>•Deliverables: In your writeup, include the best hyperparameters you used (e.g., training schedule, number of iterations, learning rate, backprop timesteps), your saved model parameters for your best model, and your evaluation result.</p><p><strong>Problem 1:</strong></p><p>•Due Date: <strong>Sep. 28, 2021</strong> (<strong>Tuesday</strong>)</p><p>•Task: handwritten digits recognition</p><p>•Data:  <a href="http://yann.lecun.com/exdb/mnist/">MNIST data set</a></p><p>•In this assignment you will practice putting together a simple image classification pipeline, based on the Softmax and the fully-connected classifier. The goals of this assignment are as follows:</p><p>– understand the basic <strong>Image Classification pipeline</strong> and the data-driven approach (train/predict stages)</p><p>– understand the train/val/test <strong>splits</strong> and the use of validation data for <strong>hyperparameter</strong> <strong>tuning</strong></p><p>– implement and apply a <strong>Softmax</strong> classifier</p><p>– implement and apply a <strong>Fully-connected neural network</strong> classifier</p><p>– understand the differences and tradeoffs between these classifiers</p><p>– implement various <strong>update rules</strong> used to optimize Neural Networks</p><h4 id><a href="#" class="headerlink" title></a></h4><p>•Do something extra! </p><p>​        – Maybe you can experiment with a different loss function and regularization? </p><p>​        – Or maybe you can experiment with different optimization algorithm (e.g., batch GD, online GD, mini-batch GD, SGD, or other optimization alg., e.g., Momentum, Adsgrad, Adam, Admax)</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
