<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>AVS增强&amp;分离论文list | 2021_timeMachine</title>
  <meta name="author" content="张靖元">
  
  <meta name="description" content="daydayup">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="AVS增强&amp;分离论文list"/>
  <meta property="og:site_name" content="2021_timeMachine"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/EskimoA000.github.io/atom.xml" title="2021_timeMachine" type="application/atom+xml">
  
  
    <link href="/EskimoA000.github.io/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/EskimoA000.github.io/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/EskimoA000.github.io/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  




<meta name="generator" content="Hexo 5.4.0"></head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/EskimoA000.github.io/">2021_timeMachine</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/EskimoA000.github.io/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/EskimoA000.github.io/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/EskimoA000.github.io/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/EskimoA000.github.io/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> AVS增强&amp;分离论文list</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <h2 id="Audio-Visual-Speech-Enhancement-and-Separation"><a href="#Audio-Visual-Speech-Enhancement-and-Separation" class="headerlink" title="Audio-Visual Speech Enhancement and Separation"></a>Audio-Visual Speech Enhancement and Separation</h2><ul>
<li>A. Adeel, J. Ahmad, H. Larijani, and A. Hussain, “A novel real-time, lightweight chaotic-encryption scheme for next-generation audio-visual hearing aids,” Cognitive Computation, vol. 12, no. 3, pp. 589–601, 2019. [<a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007%2Fs12559-019-09653-z">paper]</a></li>
<li>A. Adeel, M. Gogate, and A. Hussain, “Towards next-generation lip-reading driven hearing-aids: A preliminary prototype demo,” in Proc. of CHAT, 2017. [<a target="_blank" rel="noopener" href="http://spandh.dcs.shef.ac.uk/chat2017/papers/CHAT_2017_adeel.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://cogbid.github.io/cogavhearingdemo/">demo]</a></li>
<li>A. Adeel, M. Gogate, and A. Hussain, “Contextual deep learning-based audio-visual switching for speech enhancement in real-world environments,” Information Fusion, vol. 59, pp. 163–170, 2020. [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1566253518306018">paper]</a></li>
<li>A. Adeel, M. Gogate, A. Hussain, and W. M. Whitmer, “Lip-reading driven deep learning approach for speech enhancement,” IEEE Transactions on Emerging Topics in Computational Intelligence, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.00046.pdf">paper]</a></li>
<li>T. Afouras, J. S. Chung, and A. Zisserman, “The conversation: Deep audio-visual speech enhancement,” Proc. of Interspeech, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.04121">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/demo/theconversation/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2TWotLwutkI&feature=youtu.be">demo 1]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/demo/theconversation/">other demos]</a></li>
<li>T. Afouras, J. S. Chung, and A. Zisserman, “My lips are concealed: Audio-visual speech enhancement through obstructions,” in Proc. of Interspeech, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.04975">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/research/concealed">project page]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/research/concealed/">demo]</a></li>
<li>Z. Aldeneh, A. P. Kumar, B.-J. Theobald, E. Marchi, S. Kajarekar, D. Naik, and A. H. Abdelaziz, “Self-supervised learning of visual speech features with audiovisual speech enhancement,” arXiv preprint arXiv:2004.12031, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.12031">paper]</a></li>
<li>A. Arriandiaga, G. Morrone, L. Pasa, L. Badino, and C. Bartolozzi, “Audio-visual target speaker extraction on multi-talker environment using event-driven cameras,” arXiv preprint arXiv:1912.02671, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.02671.pdf">paper]</a></li>
<li>S.-Y. Chuang, Y. Tsao, C.-C. Lo, and H.-M. Wang, “Lite audio-visual speech enhancement,” in Proc. of Interspeech (to appear), 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.11769">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/kagaminccino/LAVSE">code]</a></li>
<li>H. Chen, J. Du, Y. Hu, L.-R. Dai, B.-C. Yin, C.-H. Lee, “Correlating subword articulation with lip shapes for embedding aware audio-visual speech enhancement ,” in Neural Network, vol. 143, pp. 171-182, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.09561.pdf">paper]</a> *</li>
<li>S.-W. Chung, S. Choe, J. S. Chung, and H.-G. Kang, “Facefilter: Audio-visual speech separation using still images,” arXiv preprint arXiv:2005.07074, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.07074.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://youtu.be/ku9xoLh62E4">demo]</a></li>
<li>A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein, “Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation,” ACM Transactions on Graphics, vol. 37, no. 4, pp. 112:1–112:11, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.03619.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://looking-to-listen.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=rVQVAPiJWKU&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://looking-to-listen.github.io/supplemental/index.html">supplementary material]</a></li>
<li>A. Gabbay, A. Ephrat, T. Halperin, and S. Peleg, “Seeing through noise: Visually driven speaker separation and enhancement,” in Proc. of ICASSP, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.06767.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.vision.huji.ac.il/speaker-separation/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=qmsyj7vAzoI&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/avivga/cocktail-party">code]</a></li>
<li>A. Gabbay, A. Shamir, and S. Peleg, “Visual speech enhancement,” in Proc. of Interspeech, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.08789.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.vision.huji.ac.il/visual-speech-enhancement/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=nyYarDGpcYA&feature=emb_title">demo 1]</a> [<a target="_blank" rel="noopener" href="http://www.vision.huji.ac.il/visual-speech-enhancement/">other demos]</a> [<a target="_blank" rel="noopener" href="https://github.com/avivga/audio-visual-speech-enhancement">code]</a></li>
<li>R. Gao and K. Grauman, “VISUALVOICE: Audio-visual speech separation with cross-modal consistency,” in Proc. of CVPR, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.03149.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://vision.cs.utexas.edu/projects/VisualVoice/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=tNR9QD6IN8c">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/VisualVoice">code]</a> [<a target="_blank" rel="noopener" href="http://vision.cs.utexas.edu/projects/VisualVoice/VisualVoice_Supp.pdf">supplementary material]</a> *</li>
<li>M. Gogate, A. Adeel, R. Marxer, J. Barker, and A. Hussain, “DNN driven speaker independent audio-visual mask estimation for speech separation,” in Proc. of Interspeech, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.00060.pdf">paper]</a></li>
<li>M. Gogate, K. Dashtipour, A. Adeel, and A. Hussain, “Cochleanet: A robust language-independent audio-visual model for speech enhancement,” Information Fusion, vol. 63, pp. 273–285, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.10407.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://cochleanet.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://vimeo.com/357546330">demo]</a> [<a target="_blank" rel="noopener" href="https://cochleanet.github.io/supplementary/">supplementary material]</a></li>
<li>M. Gogate, K. Dashtipour, and A. Hussain, “Towards Robust Real-time Audio-Visual Speech Enhancement,” arXiv preprint arXiv:2112.09060, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.09060.pdf">paper]</a> *</li>
<li>R. Gu, S.-X. Zhang, Y. Xu, L. Chen, Y. Zou, and D. Yu, “Multi-modal multi-channel target speech separation,” IEEE Journal of Selected Topics in Signal Processing, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.07032.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://moplast.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://moplast.github.io/">demo]</a></li>
<li>J.-C. Hou, S.-S. Wang, Y.-H. Lai, Y. Tsao, H.-W. Chang, and H.- M. Wang, “Audio-visual speech enhancement using multimodal deep convolutional neural networks,” IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 2, no. 2, pp. 117–128, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.10893.pdf">paper]</a></li>
<li>J.-C. Hou, S.-S. Wang, Y.-H. Lai, J.-C. Lin, Y. Tsao, H.-W. Chang, and H.-M. Wang, “Audio-visual speech enhancement using deep neural networks,” in Proc. of APSIPA, 2016. [<a target="_blank" rel="noopener" href="https://www.citi.sinica.edu.tw/papers/yu.tsao/5427-F.pdf">paper]</a></li>
<li>A. Hussain, J. Barker, R. Marxer, A. Adeel, W. Whitmer, R. Watt, and P. Derleth, “Towards multi-modal hearing aid design and evaluation in realistic audio-visual settings: Challenges and opportunities,” in Proc. of CHAT, 2017. [<a target="_blank" rel="noopener" href="http://spandh.dcs.shef.ac.uk/chat2017/papers/CHAT_2017_hussain.pdf">paper]</a></li>
<li>T. Hussain, M. Gogate, K. Dashtipour, and A. Hussain, “Towards intelligibility-oriented audio-visual speech enhancement,” arXiv preprint arXiv:2111.09642, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.09642.pdf">paper]</a> *</li>
<li>E. Ideli, “Audio-visual speech processing using deep learning techniques.” MSc thesis, Applied Sciences: School of Engineering Science, 2019. [<a target="_blank" rel="noopener" href="https://summit.sfu.ca/item/19744">paper]</a></li>
<li>E. Ideli, B. Sharpe, I. V. Bajić, and R. G. Vaughan,“Visually assisted time-domain speech enhancement,” in Proc. of GlobalSIP, 2019. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8969244">paper]</a></li>
<li>B. İnan, M. Cernak, H. Grabner, H. P. Tukuljac, R. C. Pena, and B. Ricaud, “Evaluating audiovisual source separation in the context of video conferencing,” Proc. of Interspeech, 2019. [<a target="_blank" rel="noopener" href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2671.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/berkayinan/audiovisual-separation-for-vc">code]</a></li>
<li>K. Ito, M. Yamamoto, and K. Nagamatsu, “Audio-visual speech enhancement method conditioned in the lip motion and speaker-discriminative embeddings,” Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9414133">paper]</a> *</li>
<li>M. L. Iuzzolino and K. Koishida, “AV(SE)²: Audio-visual squeeze- excite speech enhancement,” in Proc. of ICASSP. IEEE, 2020, pp. 7539–7543. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9054528">paper]</a></li>
<li>H. R. V. Joze, A. Shaban, M. L. Iuzzolino, and K. Koishida, “MMTM: Multimodal transfer module for CNN fusion,” Proc. of CVPR, 2020. [<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Joze_MMTM_Multimodal_Transfer_Module_for_CNN_Fusion_CVPR_2020_paper.pdf">paper]</a></li>
<li>F. U. Khan, B. P. Milner, and T. Le Cornu, “Using visual speech information in masking methods for audio speaker separation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1742–1754, 2018. [<a target="_blank" rel="noopener" href="https://ueaeprints.uea.ac.uk/id/eprint/67404/1/ieee_speaker_separation_2015_v4.0.pdf">paper]</a></li>
<li>C. Li and Y. Qian, “Deep audio-visual speech separation with attention mechanism,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9054180">paper]</a></li>
<li>Y. Li, Z. Liu, Y. Na, Z. Wang, B. Tian, and Q. Fu, “A visual-pilot deep fusion for target speech separation in multitalker noisy environment,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9054263">paper]</a></li>
<li>R. Lu, Z. Duan, and C. Zhang, “Listen and look: Audio–visual matching assisted speech source separation,” IEEE Signal Processing Letters, vol. 25, no. 9, pp. 1315–1319, 2018. [<a target="_blank" rel="noopener" href="http://www2.ece.rochester.edu/projects/air/publications/lu2018listen.pdf">paper]</a></li>
<li>R. Lu, Z. Duan, and C. Zhang, “Audio–visual deep clustering for speech separation, ”IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 11, pp. 1697–1712, 2019. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8762221">paper]</a></li>
<li>Y. Luo, J. Wang, X. Wang, L. Wen, and L. Wang, “Audio-visual speech separation using i-Vectors,” in Proc. of ICICSP, 2019. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8958547">paper]</a></li>
<li>N. Makishima, M. Ihori, A. Takashima, T. Tanaka, S. Orihashi, and R. Masumura, “Audio-visual speech separation using cross-modal correspondence loss,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.01463.pdf">paper]</a> *</li>
<li>D. Michelsanti, Z.-H. Tan, S. Sigurdsson, and J. Jensen, “On training targets and objective functions for deep-learning-based audio-visual speech enhancement,” in Proc. of ICASSP, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.06234.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://kom.aau.dk/~zt/online/icassp2019_sup_mat.pdf">supplementary material]</a></li>
<li>D. Michelsanti, Z.-H. Tan, S. Sigurdsson, and J. Jensen, “Deep- learning-based audio-visual speech enhancement in presence of Lombard effect,” Speech Communication, vol. 115, pp. 38–50, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.12605.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=IRlaU0EMeOg">demo]</a></li>
<li>D. Michelsanti, Z.-H. Tan, S. Sigurdsson, and J. Jensen, “Effects of Lombard reflex on the performance of deep-learning-based audio-visual speech enhancement systems,” in Proc. of ICASSP, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.06250.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://vbn.aau.dk/en/activities/demo-effects-of-lombard-reflex-on-deep-learning-based-audio-visua">demo]</a></li>
<li>J. F. Montesinos, V. S. Kadandale, and G. Haro, “VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer,” arXiv preprint arXiv:2203.04099, 2022. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.04099.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://ipcv.github.io/VoViT/demos/">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/JuanFMontesinos/VoViT">code]</a> [<a target="_blank" rel="noopener" href="https://ipcv.github.io/VoViT/">project page]</a> *</li>
<li>G. Morrone, S. Bergamaschi, L. Pasa, L. Fadiga, V. Tikhanoff, and L. Badino, “Face landmark-based speaker-independent audio-visual speech enhancement in multi-talker environments,” in Proc. of ICASSP, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.02480.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://dr-pato.github.io/audio_visual_speech_enhancement/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=YQ0q-OFphKM&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://dr-pato.github.io/audio_visual_speech_enhancement/">other demos]</a> [<a target="_blank" rel="noopener" href="https://github.com/dr-pato/audio_visual_speech_enhancement">code]</a></li>
<li>T. Ochiai, M. Delcroix, K. Kinoshita, A. Ogawa, and T. Nakatani, “Multimodal SpeakerBeam: Single channel target speech extraction with audio-visual speaker clues,” Proc. Interspeech, 2019. [<a target="_blank" rel="noopener" href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1513.pdf">paper]</a></li>
<li>A. Owens and A. A. Efros, “Audio-visual scene analysis with self-supervised multisensory features,” in Proc. of ECCV, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.03641.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://andrewowens.com/multisensory">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=rwCIRu_hAJ8&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/andrewowens/multisensory">code]</a></li>
<li>Z. Pan, M. Ge and H. Li, “USEV: Universal speaker extraction with visual cue,” 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.14831.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/zexupan/USEV">code]</a> *</li>
<li>Z. Pan, R. Tao, C. Xu and H. Li, “MuSe: Multi-modal target speaker extraction with visual cues,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.07775.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/zexupan/MuSE">code]</a> *</li>
<li>Z. Pan, R. Tao, C. Xu and H. Li, “Selective Hearing through Lip-reading,” arXiv preprint arXiv:2106.07150, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.07150.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/zexupan/reentry">code]</a> *</li>
<li>L. Pasa, G. Morrone, and L. Badino, “An analysis of speech enhancement and recognition losses in limited resources multi-talker single channel audio-visual ASR,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.08248.pdf">paper]</a></li>
<li>L. Qu, C. Weber, and S. Wermter, “Multimodal target speech separation with voice and face references,” arXiv preprint arXiv:2005.08335, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.08335.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/danmic/av-se/blob/master/leyuanqu.github.io/INTERSPEECH2020">project page]</a> [<a target="_blank" rel="noopener" href="https://github.com/danmic/av-se/blob/master/leyuanqu.github.io/INTERSPEECH2020">demo]</a></li>
<li>M. Sadeghi and X. Alameda-Pineda, “Mixture of inference networks for VAE-based audio-visual speech enhancement,” arXiv preprint arXiv:1912.10647, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.10647.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/min-vae-se/">project page]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/min-vae-se/#audio">demo]</a> [<a target="_blank" rel="noopener" href="https://gitlab.inria.fr/smostafa/avse-vae">code]</a></li>
<li>M. Sadeghi and X. Alameda-Pineda, “Robust unsupervised audio-visual speech enhancement using a mixture of variational autoencoders,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.03930.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/vae-mm-se/">project page]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/files/2019/10/vae_mm_supp.pdf">supplementary material]</a> [<a target="_blank" rel="noopener" href="https://gitlab.inria.fr/smostafa/avse-vae">code]</a></li>
<li>M. Sadeghi and X. Alameda-Pineda, “Switching variational auto-encoders for noise-agnostic audio-visual speech enhancement,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.04144.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/swvae/">project page]</a> *</li>
<li>M. Sadeghi, S. Leglaive, X. Alameda-Pineda, L. Girin, and R. Horaud, “Audio-visual speech enhancement using conditional variational autoencoders,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 1788–1800, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.02590.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/av-vae-se/">project page]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/av-vae-se/">demo]</a> [<a target="_blank" rel="noopener" href="https://gitlab.inria.fr/smostafa/avse-vae">code]</a></li>
<li>H. Sato, T. Ochiai, K. Kinoshita, M. Delcroix, T. Nakatani and S. Araki. “Multimodal attention fusion for target speaker extraction,” in Proc. of SLT, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.01326.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.kecl.ntt.co.jp/icl/signal/member/demo/audio_visual_speakerbeam.html">project page]</a> [<a target="_blank" rel="noopener" href="http://www.kecl.ntt.co.jp/icl/signal/member/demo/audio_visual_speakerbeam.html">demo]</a> *</li>
<li>S. S. Shetu, S. Chakrabarty, and E. A. P. Habets, “An empirical study of visual features for DNN based audio-visual speech enhancement in multi-talker environments,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.04359.pdf">paper]</a> *</li>
<li>Z. Sun, Y. Wang, and L. Cao, “An attention based speaker-independent audio-visual deep learning model for speech enhancement,” in Proc. of MMM, 2020. [<a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-030-37734-2_60">paper]</a></li>
<li>K. Tan, Y. Xu, S.-X. Zhang, M. Yu, and D. Yu, “Audio-visual speech separation and dereverberation with a two-stage multimodal network,” IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 3, pp. 542–553, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.07352.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://jupiterethan.github.io/av-enh.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://jupiterethan.github.io/av-enh.github.io/">demo]</a></li>
<li>W. Wang, C. Xing, D. Wang, X. Chen, and F. Sun, “A robust audio-visual speech enhancement model,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9053033">paper]</a></li>
<li>J. Wu, Y. Xu, S.-X. Zhang, L.-W. Chen, M. Yu, L. Xie, and D. Yu, “Time domain audio visual speech separation,” in Proc. of ASRU, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03760.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://funcwj.github.io/online-demo/page/tavs">project page]</a> [<a target="_blank" rel="noopener" href="https://funcwj.github.io/online-demo/page/tavs">demo]</a></li>
<li>Z. Wu, S. Sivadas, Y. K. Tan, M. Bin, and R. S. M. Goh,“Multi-modal hybrid deep neural network for speech enhancement,” arXiv preprint arXiv:1606.04750, 2016. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.04750.pdf">paper]</a></li>
<li>X. Xu, Y. Wang, D. Xu, C. Zhang, Y. Peng, J. Jia, and B. Chen, “VSEGAN: Visual speech enhancement generative adversarial network,” arXiv preprint arXiv:2102.02599, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.02599.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://xinmengxu.github.io/AVSE/VSEGAN">project page]</a> *</li>
<li>X. Xu, Y. Wang, D. Xu, C. Zhang, Y. Peng, J. Jia, and B. Chen, “AMFFCN: Attentional multi-layer feature fusion convolution network for audio-visual speech enhancement,” arXiv preprint arXiv:2101.06268, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.06268.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://xinmengxu.github.io/AVSE/AMFFCN">project page]</a> *</li>
<li>Y. Xu, M. Yu, S.-X. Zhang, L. Chen, C. Weng, J. Liu, and D. Yu, “Neural spatio-temporal beamformer for target speech separation,” Proc. of Interspeech (to appear), 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.03889.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://yongxuustc.github.io/mtmvdr">project page]</a> [<a target="_blank" rel="noopener" href="https://yongxuustc.github.io/mtmvdr">demo]</a></li>
</ul>

	  <div class="article-footer-copyright">

    本博客采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议(CC BY-NC-SA 4.0) 发布.</a>
</div>

	</div>

	
	<span id="/EskimoA000.github.io/2022/03/15/AVS%E5%A2%9E%E5%BC%BA-%E5%88%86%E7%A6%BB/" class="leancloud-visitors view" data-flag-title="AVS增强&amp;分离论文list">
		<em class="post-meta-item-text"> Page View </em> <i class="leancloud-visitors-count"></i>
	</span>
	
	<div>
  	<center>

	<div class="pagination">

    
    
    <a type="button" class="btn btn-default disabled"><i class="fa fa-arrow-circle-o-left"></i>上一页</a>
    

    <a href="/EskimoA000.github.io/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/EskimoA000.github.io/2022/03/15/Speech-Assessment/" type="button" class="btn btn-default ">下一页<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>


    </center>
	</div>
	
	<!-- comment -->
	<!--
<section id="comment">
    <h2 class="title">留言</h2>

    
</section>

-->
	
		<section id="comments" class="comments">
			<style>
			.comments{margin:30px;padding:10px;background:rgb(0, 0, 0)}
			@media screen and (max-width:800px){.comments{margin:auto;padding:10px;background:#000}}
			</style>
			<div id="vcomment" class="comment"></div>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script src="https://cdnjs.loli.net/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
<script>
var valineConfig = {"enable":true,"appId":"xx","appKey":"xx","placeholder":"提交评论时留下邮箱收到回复后将自动通知","visitor":true,"avatar":"monsterid","requiredFields":["nick","mail"]}
valineConfig.el='#vcomment';
new Valine(valineConfig);
    // new Valine({
    //     el: '#vcomment',
    //     appId: "",
    //     appKey: "",
    //     placeholder: "提交评论时留下邮箱收到回复后将自动通知",
    //     avatar:"monsterid",
    //     visitor: "true",
    //     requiredFields: "nick,mail".split(','),
    // });
</script>

		</section>
	
	</div> <!-- col-md-9/col-md-12 -->


	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2022-03-15 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/EskimoA000.github.io/categories/多模态语音降噪与分离/">多模态语音降噪与分离<span>1</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/EskimoA000.github.io/tags/多模态语音降噪与分离论文集/">多模态语音降噪与分离论文集<span>1</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

		

	</div>
	
		

</div><!-- row -->

<!--
 -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  
  &copy; 2022 张靖元's Blog
  
      powered by <a href="http://hexo.io/" target="_blank">Hexo</a>.Theme <a href="https://github.com/Ares-X/hexo-theme-freemind.bithack" target="_blank">freemind.bithack</a>  
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/EskimoA000.github.io/js/jquery.imagesloaded.min.js"></script>
<script src="/EskimoA000.github.io/js/gallery.js"></script>
<script src="/EskimoA000.github.io/js/bootstrap.min.js"></script>
<script src="/EskimoA000.github.io/js/main.js"></script>
<script src="/EskimoA000.github.io/js/search.js"></script> 


<link rel="stylesheet" href="/EskimoA000.github.io/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/EskimoA000.github.io/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/EskimoA000.github.io/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

<script src="/EskimoA000.github.io/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/EskimoA000.github.io/live2dw/assets/assets/wanko.model.json"},"display":{"position":"right","width":260,"height":600},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.8},"log":false});</script></body>
   </html>
