<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>2021_timeMachine</title>
  <meta name="author" content="张靖元">
  
  <meta name="description" content="daydayup">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="2021_timeMachine"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/EskimoA000.github.io/atom.xml" title="2021_timeMachine" type="application/atom+xml">
  
  
    <link href="/EskimoA000.github.io/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/EskimoA000.github.io/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/EskimoA000.github.io/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/EskimoA000.github.io/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  




<meta name="generator" content="Hexo 5.4.0"></head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/EskimoA000.github.io/">2021_timeMachine</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/EskimoA000.github.io/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/EskimoA000.github.io/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/EskimoA000.github.io/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/EskimoA000.github.io/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 <div class="page-header logo">
  <h1>zjy的Cyberspace<span class="blink-fast">|</span></h1>
</div>

<div class="row page">

	
	<div class="col-md-9">
	

		<div class="slogan">


		<i class="fa fa-heart blink-slow"></i>

		流 年 笑 掷 :) 未 来 可 期

</div>    

		<div id="top_search"></div>
		<div class="mypage">
		
		<!-- title and entry -->
		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/EskimoA000.github.io/2022/03/15/AVS增强-分离/" >AVS增强&amp;分离论文list</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2022-03-15  </div>
		</div>
	</div>
	


			<!--
<div class="entry">
  <div class="row">
	
	
		<h2 id="Audio-Visual-Speech-Enhancement-and-Separation"><a href="#Audio-Visual-Speech-Enhancement-and-Separation" class="headerlink" title="Audio-Visual Speech Enhancement and Separation"></a>Audio-Visual Speech Enhancement and Separation</h2><ul>
<li>A. Adeel, J. Ahmad, H. Larijani, and A. Hussain, “A novel real-time, lightweight chaotic-encryption scheme for next-generation audio-visual hearing aids,” Cognitive Computation, vol. 12, no. 3, pp. 589–601, 2019. [<a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007%2Fs12559-019-09653-z">paper]</a></li>
<li>A. Adeel, M. Gogate, and A. Hussain, “Towards next-generation lip-reading driven hearing-aids: A preliminary prototype demo,” in Proc. of CHAT, 2017. [<a target="_blank" rel="noopener" href="http://spandh.dcs.shef.ac.uk/chat2017/papers/CHAT_2017_adeel.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://cogbid.github.io/cogavhearingdemo/">demo]</a></li>
<li>A. Adeel, M. Gogate, and A. Hussain, “Contextual deep learning-based audio-visual switching for speech enhancement in real-world environments,” Information Fusion, vol. 59, pp. 163–170, 2020. [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1566253518306018">paper]</a></li>
<li>A. Adeel, M. Gogate, A. Hussain, and W. M. Whitmer, “Lip-reading driven deep learning approach for speech enhancement,” IEEE Transactions on Emerging Topics in Computational Intelligence, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.00046.pdf">paper]</a></li>
<li>T. Afouras, J. S. Chung, and A. Zisserman, “The conversation: Deep audio-visual speech enhancement,” Proc. of Interspeech, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.04121">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/demo/theconversation/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2TWotLwutkI&feature=youtu.be">demo 1]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/demo/theconversation/">other demos]</a></li>
<li>T. Afouras, J. S. Chung, and A. Zisserman, “My lips are concealed: Audio-visual speech enhancement through obstructions,” in Proc. of Interspeech, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.04975">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/research/concealed">project page]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/research/concealed/">demo]</a></li>
<li>Z. Aldeneh, A. P. Kumar, B.-J. Theobald, E. Marchi, S. Kajarekar, D. Naik, and A. H. Abdelaziz, “Self-supervised learning of visual speech features with audiovisual speech enhancement,” arXiv preprint arXiv:2004.12031, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.12031">paper]</a></li>
<li>A. Arriandiaga, G. Morrone, L. Pasa, L. Badino, and C. Bartolozzi, “Audio-visual target speaker extraction on multi-talker environment using event-driven cameras,” arXiv preprint arXiv:1912.02671, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.02671.pdf">paper]</a></li>
<li>S.-Y. Chuang, Y. Tsao, C.-C. Lo, and H.-M. Wang, “Lite audio-visual speech enhancement,” in Proc. of Interspeech (to appear), 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.11769">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/kagaminccino/LAVSE">code]</a></li>
<li>H. Chen, J. Du, Y. Hu, L.-R. Dai, B.-C. Yin, C.-H. Lee, “Correlating subword articulation with lip shapes for embedding aware audio-visual speech enhancement ,” in Neural Network, vol. 143, pp. 171-182, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.09561.pdf">paper]</a> *</li>
<li>S.-W. Chung, S. Choe, J. S. Chung, and H.-G. Kang, “Facefilter: Audio-visual speech separation using still images,” arXiv preprint arXiv:2005.07074, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.07074.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://youtu.be/ku9xoLh62E4">demo]</a></li>
<li>A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein, “Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation,” ACM Transactions on Graphics, vol. 37, no. 4, pp. 112:1–112:11, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.03619.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://looking-to-listen.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=rVQVAPiJWKU&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://looking-to-listen.github.io/supplemental/index.html">supplementary material]</a></li>
<li>A. Gabbay, A. Ephrat, T. Halperin, and S. Peleg, “Seeing through noise: Visually driven speaker separation and enhancement,” in Proc. of ICASSP, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.06767.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.vision.huji.ac.il/speaker-separation/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=qmsyj7vAzoI&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/avivga/cocktail-party">code]</a></li>
<li>A. Gabbay, A. Shamir, and S. Peleg, “Visual speech enhancement,” in Proc. of Interspeech, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.08789.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.vision.huji.ac.il/visual-speech-enhancement/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=nyYarDGpcYA&feature=emb_title">demo 1]</a> [<a target="_blank" rel="noopener" href="http://www.vision.huji.ac.il/visual-speech-enhancement/">other demos]</a> [<a target="_blank" rel="noopener" href="https://github.com/avivga/audio-visual-speech-enhancement">code]</a></li>
<li>R. Gao and K. Grauman, “VISUALVOICE: Audio-visual speech separation with cross-modal consistency,” in Proc. of CVPR, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.03149.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://vision.cs.utexas.edu/projects/VisualVoice/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=tNR9QD6IN8c">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/VisualVoice">code]</a> [<a target="_blank" rel="noopener" href="http://vision.cs.utexas.edu/projects/VisualVoice/VisualVoice_Supp.pdf">supplementary material]</a> *</li>
<li>M. Gogate, A. Adeel, R. Marxer, J. Barker, and A. Hussain, “DNN driven speaker independent audio-visual mask estimation for speech separation,” in Proc. of Interspeech, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.00060.pdf">paper]</a></li>
<li>M. Gogate, K. Dashtipour, A. Adeel, and A. Hussain, “Cochleanet: A robust language-independent audio-visual model for speech enhancement,” Information Fusion, vol. 63, pp. 273–285, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.10407.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://cochleanet.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://vimeo.com/357546330">demo]</a> [<a target="_blank" rel="noopener" href="https://cochleanet.github.io/supplementary/">supplementary material]</a></li>
<li>M. Gogate, K. Dashtipour, and A. Hussain, “Towards Robust Real-time Audio-Visual Speech Enhancement,” arXiv preprint arXiv:2112.09060, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.09060.pdf">paper]</a> *</li>
<li>R. Gu, S.-X. Zhang, Y. Xu, L. Chen, Y. Zou, and D. Yu, “Multi-modal multi-channel target speech separation,” IEEE Journal of Selected Topics in Signal Processing, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.07032.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://moplast.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://moplast.github.io/">demo]</a></li>
<li>J.-C. Hou, S.-S. Wang, Y.-H. Lai, Y. Tsao, H.-W. Chang, and H.- M. Wang, “Audio-visual speech enhancement using multimodal deep convolutional neural networks,” IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 2, no. 2, pp. 117–128, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.10893.pdf">paper]</a></li>
<li>J.-C. Hou, S.-S. Wang, Y.-H. Lai, J.-C. Lin, Y. Tsao, H.-W. Chang, and H.-M. Wang, “Audio-visual speech enhancement using deep neural networks,” in Proc. of APSIPA, 2016. [<a target="_blank" rel="noopener" href="https://www.citi.sinica.edu.tw/papers/yu.tsao/5427-F.pdf">paper]</a></li>
<li>A. Hussain, J. Barker, R. Marxer, A. Adeel, W. Whitmer, R. Watt, and P. Derleth, “Towards multi-modal hearing aid design and evaluation in realistic audio-visual settings: Challenges and opportunities,” in Proc. of CHAT, 2017. [<a target="_blank" rel="noopener" href="http://spandh.dcs.shef.ac.uk/chat2017/papers/CHAT_2017_hussain.pdf">paper]</a></li>
<li>T. Hussain, M. Gogate, K. Dashtipour, and A. Hussain, “Towards intelligibility-oriented audio-visual speech enhancement,” arXiv preprint arXiv:2111.09642, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.09642.pdf">paper]</a> *</li>
<li>E. Ideli, “Audio-visual speech processing using deep learning techniques.” MSc thesis, Applied Sciences: School of Engineering Science, 2019. [<a target="_blank" rel="noopener" href="https://summit.sfu.ca/item/19744">paper]</a></li>
<li>E. Ideli, B. Sharpe, I. V. Bajić, and R. G. Vaughan,“Visually assisted time-domain speech enhancement,” in Proc. of GlobalSIP, 2019. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8969244">paper]</a></li>
<li>B. İnan, M. Cernak, H. Grabner, H. P. Tukuljac, R. C. Pena, and B. Ricaud, “Evaluating audiovisual source separation in the context of video conferencing,” Proc. of Interspeech, 2019. [<a target="_blank" rel="noopener" href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2671.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/berkayinan/audiovisual-separation-for-vc">code]</a></li>
<li>K. Ito, M. Yamamoto, and K. Nagamatsu, “Audio-visual speech enhancement method conditioned in the lip motion and speaker-discriminative embeddings,” Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9414133">paper]</a> *</li>
<li>M. L. Iuzzolino and K. Koishida, “AV(SE)²: Audio-visual squeeze- excite speech enhancement,” in Proc. of ICASSP. IEEE, 2020, pp. 7539–7543. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9054528">paper]</a></li>
<li>H. R. V. Joze, A. Shaban, M. L. Iuzzolino, and K. Koishida, “MMTM: Multimodal transfer module for CNN fusion,” Proc. of CVPR, 2020. [<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Joze_MMTM_Multimodal_Transfer_Module_for_CNN_Fusion_CVPR_2020_paper.pdf">paper]</a></li>
<li>F. U. Khan, B. P. Milner, and T. Le Cornu, “Using visual speech information in masking methods for audio speaker separation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1742–1754, 2018. [<a target="_blank" rel="noopener" href="https://ueaeprints.uea.ac.uk/id/eprint/67404/1/ieee_speaker_separation_2015_v4.0.pdf">paper]</a></li>
<li>C. Li and Y. Qian, “Deep audio-visual speech separation with attention mechanism,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9054180">paper]</a></li>
<li>Y. Li, Z. Liu, Y. Na, Z. Wang, B. Tian, and Q. Fu, “A visual-pilot deep fusion for target speech separation in multitalker noisy environment,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9054263">paper]</a></li>
<li>R. Lu, Z. Duan, and C. Zhang, “Listen and look: Audio–visual matching assisted speech source separation,” IEEE Signal Processing Letters, vol. 25, no. 9, pp. 1315–1319, 2018. [<a target="_blank" rel="noopener" href="http://www2.ece.rochester.edu/projects/air/publications/lu2018listen.pdf">paper]</a></li>
<li>R. Lu, Z. Duan, and C. Zhang, “Audio–visual deep clustering for speech separation, ”IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 11, pp. 1697–1712, 2019. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8762221">paper]</a></li>
<li>Y. Luo, J. Wang, X. Wang, L. Wen, and L. Wang, “Audio-visual speech separation using i-Vectors,” in Proc. of ICICSP, 2019. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8958547">paper]</a></li>
<li>N. Makishima, M. Ihori, A. Takashima, T. Tanaka, S. Orihashi, and R. Masumura, “Audio-visual speech separation using cross-modal correspondence loss,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.01463.pdf">paper]</a> *</li>
<li>D. Michelsanti, Z.-H. Tan, S. Sigurdsson, and J. Jensen, “On training targets and objective functions for deep-learning-based audio-visual speech enhancement,” in Proc. of ICASSP, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.06234.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://kom.aau.dk/~zt/online/icassp2019_sup_mat.pdf">supplementary material]</a></li>
<li>D. Michelsanti, Z.-H. Tan, S. Sigurdsson, and J. Jensen, “Deep- learning-based audio-visual speech enhancement in presence of Lombard effect,” Speech Communication, vol. 115, pp. 38–50, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.12605.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=IRlaU0EMeOg">demo]</a></li>
<li>D. Michelsanti, Z.-H. Tan, S. Sigurdsson, and J. Jensen, “Effects of Lombard reflex on the performance of deep-learning-based audio-visual speech enhancement systems,” in Proc. of ICASSP, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.06250.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://vbn.aau.dk/en/activities/demo-effects-of-lombard-reflex-on-deep-learning-based-audio-visua">demo]</a></li>
<li>J. F. Montesinos, V. S. Kadandale, and G. Haro, “VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer,” arXiv preprint arXiv:2203.04099, 2022. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.04099.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://ipcv.github.io/VoViT/demos/">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/JuanFMontesinos/VoViT">code]</a> [<a target="_blank" rel="noopener" href="https://ipcv.github.io/VoViT/">project page]</a> *</li>
<li>G. Morrone, S. Bergamaschi, L. Pasa, L. Fadiga, V. Tikhanoff, and L. Badino, “Face landmark-based speaker-independent audio-visual speech enhancement in multi-talker environments,” in Proc. of ICASSP, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.02480.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://dr-pato.github.io/audio_visual_speech_enhancement/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=YQ0q-OFphKM&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://dr-pato.github.io/audio_visual_speech_enhancement/">other demos]</a> [<a target="_blank" rel="noopener" href="https://github.com/dr-pato/audio_visual_speech_enhancement">code]</a></li>
<li>T. Ochiai, M. Delcroix, K. Kinoshita, A. Ogawa, and T. Nakatani, “Multimodal SpeakerBeam: Single channel target speech extraction with audio-visual speaker clues,” Proc. Interspeech, 2019. [<a target="_blank" rel="noopener" href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1513.pdf">paper]</a></li>
<li>A. Owens and A. A. Efros, “Audio-visual scene analysis with self-supervised multisensory features,” in Proc. of ECCV, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.03641.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://andrewowens.com/multisensory">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=rwCIRu_hAJ8&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/andrewowens/multisensory">code]</a></li>
<li>Z. Pan, M. Ge and H. Li, “USEV: Universal speaker extraction with visual cue,” 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.14831.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/zexupan/USEV">code]</a> *</li>
<li>Z. Pan, R. Tao, C. Xu and H. Li, “MuSe: Multi-modal target speaker extraction with visual cues,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.07775.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/zexupan/MuSE">code]</a> *</li>
<li>Z. Pan, R. Tao, C. Xu and H. Li, “Selective Hearing through Lip-reading,” arXiv preprint arXiv:2106.07150, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.07150.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/zexupan/reentry">code]</a> *</li>
<li>L. Pasa, G. Morrone, and L. Badino, “An analysis of speech enhancement and recognition losses in limited resources multi-talker single channel audio-visual ASR,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.08248.pdf">paper]</a></li>
<li>L. Qu, C. Weber, and S. Wermter, “Multimodal target speech separation with voice and face references,” arXiv preprint arXiv:2005.08335, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.08335.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/danmic/av-se/blob/master/leyuanqu.github.io/INTERSPEECH2020">project page]</a> [<a target="_blank" rel="noopener" href="https://github.com/danmic/av-se/blob/master/leyuanqu.github.io/INTERSPEECH2020">demo]</a></li>
<li>M. Sadeghi and X. Alameda-Pineda, “Mixture of inference networks for VAE-based audio-visual speech enhancement,” arXiv preprint arXiv:1912.10647, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.10647.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/min-vae-se/">project page]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/min-vae-se/#audio">demo]</a> [<a target="_blank" rel="noopener" href="https://gitlab.inria.fr/smostafa/avse-vae">code]</a></li>
<li>M. Sadeghi and X. Alameda-Pineda, “Robust unsupervised audio-visual speech enhancement using a mixture of variational autoencoders,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.03930.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/vae-mm-se/">project page]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/files/2019/10/vae_mm_supp.pdf">supplementary material]</a> [<a target="_blank" rel="noopener" href="https://gitlab.inria.fr/smostafa/avse-vae">code]</a></li>
<li>M. Sadeghi and X. Alameda-Pineda, “Switching variational auto-encoders for noise-agnostic audio-visual speech enhancement,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.04144.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/swvae/">project page]</a> *</li>
<li>M. Sadeghi, S. Leglaive, X. Alameda-Pineda, L. Girin, and R. Horaud, “Audio-visual speech enhancement using conditional variational autoencoders,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 1788–1800, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.02590.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/av-vae-se/">project page]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/av-vae-se/">demo]</a> [<a target="_blank" rel="noopener" href="https://gitlab.inria.fr/smostafa/avse-vae">code]</a></li>
<li>H. Sato, T. Ochiai, K. Kinoshita, M. Delcroix, T. Nakatani and S. Araki. “Multimodal attention fusion for target speaker extraction,” in Proc. of SLT, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.01326.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.kecl.ntt.co.jp/icl/signal/member/demo/audio_visual_speakerbeam.html">project page]</a> [<a target="_blank" rel="noopener" href="http://www.kecl.ntt.co.jp/icl/signal/member/demo/audio_visual_speakerbeam.html">demo]</a> *</li>
<li>S. S. Shetu, S. Chakrabarty, and E. A. P. Habets, “An empirical study of visual features for DNN based audio-visual speech enhancement in multi-talker environments,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.04359.pdf">paper]</a> *</li>
<li>Z. Sun, Y. Wang, and L. Cao, “An attention based speaker-independent audio-visual deep learning model for speech enhancement,” in Proc. of MMM, 2020. [<a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-030-37734-2_60">paper]</a></li>
<li>K. Tan, Y. Xu, S.-X. Zhang, M. Yu, and D. Yu, “Audio-visual speech separation and dereverberation with a two-stage multimodal network,” IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 3, pp. 542–553, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.07352.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://jupiterethan.github.io/av-enh.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://jupiterethan.github.io/av-enh.github.io/">demo]</a></li>
<li>W. Wang, C. Xing, D. Wang, X. Chen, and F. Sun, “A robust audio-visual speech enhancement model,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9053033">paper]</a></li>
<li>J. Wu, Y. Xu, S.-X. Zhang, L.-W. Chen, M. Yu, L. Xie, and D. Yu, “Time domain audio visual speech separation,” in Proc. of ASRU, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03760.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://funcwj.github.io/online-demo/page/tavs">project page]</a> [<a target="_blank" rel="noopener" href="https://funcwj.github.io/online-demo/page/tavs">demo]</a></li>
<li>Z. Wu, S. Sivadas, Y. K. Tan, M. Bin, and R. S. M. Goh,“Multi-modal hybrid deep neural network for speech enhancement,” arXiv preprint arXiv:1606.04750, 2016. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.04750.pdf">paper]</a></li>
<li>X. Xu, Y. Wang, D. Xu, C. Zhang, Y. Peng, J. Jia, and B. Chen, “VSEGAN: Visual speech enhancement generative adversarial network,” arXiv preprint arXiv:2102.02599, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.02599.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://xinmengxu.github.io/AVSE/VSEGAN">project page]</a> *</li>
<li>X. Xu, Y. Wang, D. Xu, C. Zhang, Y. Peng, J. Jia, and B. Chen, “AMFFCN: Attentional multi-layer feature fusion convolution network for audio-visual speech enhancement,” arXiv preprint arXiv:2101.06268, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.06268.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://xinmengxu.github.io/AVSE/AMFFCN">project page]</a> *</li>
<li>Y. Xu, M. Yu, S.-X. Zhang, L. Chen, C. Weng, J. Liu, and D. Yu, “Neural spatio-temporal beamformer for target speech separation,” Proc. of Interspeech (to appear), 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.03889.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://yongxuustc.github.io/mtmvdr">project page]</a> [<a target="_blank" rel="noopener" href="https://yongxuustc.github.io/mtmvdr">demo]</a></li>
</ul>

	

	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/AVS增强-分离/#more" class="btn btn-default more">阅读此文</a>
</div>

-->
<div class="entry">
  <div class="row">
	
	
		<h2 id="Audio-Visual-Speech-Enhancement-and-Separation"><a href="#Audio-Visual-Speech-Enhancement-and-Separation" class="headerlink" title="Audio-Visual Speech Enhancement and Separation"></a>Audio-Visual Speech Enhancement and Separation</h2><ul>
<li>A. Adeel, J. Ahmad, H. Larijani, and A. Hussain, “A novel real-time, lightweight chaotic-encryption scheme for next-generation audio-visual hearing aids,” Cognitive Computation, vol. 12, no. 3, pp. 589–601, 2019. [<a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007%2Fs12559-019-09653-z">paper]</a></li>
<li>A. Adeel, M. Gogate, and A. Hussain, “Towards next-generation lip-reading driven hearing-aids: A preliminary prototype demo,” in Proc. of CHAT, 2017. [<a target="_blank" rel="noopener" href="http://spandh.dcs.shef.ac.uk/chat2017/papers/CHAT_2017_adeel.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://cogbid.github.io/cogavhearingdemo/">demo]</a></li>
<li>A. Adeel, M. Gogate, and A. Hussain, “Contextual deep learning-based audio-visual switching for speech enhancement in real-world environments,” Information Fusion, vol. 59, pp. 163–170, 2020. [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1566253518306018">paper]</a></li>
<li>A. Adeel, M. Gogate, A. Hussain, and W. M. Whitmer, “Lip-reading driven deep learning approach for speech enhancement,” IEEE Transactions on Emerging Topics in Computational Intelligence, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.00046.pdf">paper]</a></li>
<li>T. Afouras, J. S. Chung, and A. Zisserman, “The conversation: Deep audio-visual speech enhancement,” Proc. of Interspeech, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.04121">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/demo/theconversation/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2TWotLwutkI&feature=youtu.be">demo 1]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/demo/theconversation/">other demos]</a></li>
<li>T. Afouras, J. S. Chung, and A. Zisserman, “My lips are concealed: Audio-visual speech enhancement through obstructions,” in Proc. of Interspeech, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.04975">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/research/concealed">project page]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/research/concealed/">demo]</a></li>
<li>Z. Aldeneh, A. P. Kumar, B.-J. Theobald, E. Marchi, S. Kajarekar, D. Naik, and A. H. Abdelaziz, “Self-supervised learning of visual speech features with audiovisual speech enhancement,” arXiv preprint arXiv:2004.12031, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.12031">paper]</a></li>
<li>A. Arriandiaga, G. Morrone, L. Pasa, L. Badino, and C. Bartolozzi, “Audio-visual target speaker extraction on multi-talker environment using event-driven cameras,” arXiv preprint arXiv:1912.02671, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.02671.pdf">paper]</a></li>
<li>S.-Y. Chuang, Y. Tsao, C.-C. Lo, and H.-M. Wang, “Lite audio-visual speech enhancement,” in Proc. of Interspeech (to appear), 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.11769">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/kagaminccino/LAVSE">code]</a></li>
<li>H. Chen, J. Du, Y. Hu, L.-R. Dai, B.-C. Yin, C.-H. Lee, “Correlating subword articulation with lip shapes for embedding aware audio-visual speech enhancement ,” in Neural Network, vol. 143, pp. 171-182, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.09561.pdf">paper]</a> *</li>
<li>S.-W. Chung, S. Choe, J. S. Chung, and H.-G. Kang, “Facefilter: Audio-visual speech separation using still images,” arXiv preprint arXiv:2005.07074, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.07074.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://youtu.be/ku9xoLh62E4">demo]</a></li>
<li>A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein, “Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation,” ACM Transactions on Graphics, vol. 37, no. 4, pp. 112:1–112:11, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.03619.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://looking-to-listen.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=rVQVAPiJWKU&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://looking-to-listen.github.io/supplemental/index.html">supplementary material]</a></li>
<li>A. Gabbay, A. Ephrat, T. Halperin, and S. Peleg, “Seeing through noise: Visually driven speaker separation and enhancement,” in Proc. of ICASSP, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.06767.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.vision.huji.ac.il/speaker-separation/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=qmsyj7vAzoI&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/avivga/cocktail-party">code]</a></li>
<li>A. Gabbay, A. Shamir, and S. Peleg, “Visual speech enhancement,” in Proc. of Interspeech, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.08789.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.vision.huji.ac.il/visual-speech-enhancement/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=nyYarDGpcYA&feature=emb_title">demo 1]</a> [<a target="_blank" rel="noopener" href="http://www.vision.huji.ac.il/visual-speech-enhancement/">other demos]</a> [<a target="_blank" rel="noopener" href="https://github.com/avivga/audio-visual-speech-enhancement">code]</a></li>
<li>R. Gao and K. Grauman, “VISUALVOICE: Audio-visual speech separation with cross-modal consistency,” in Proc. of CVPR, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.03149.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://vision.cs.utexas.edu/projects/VisualVoice/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=tNR9QD6IN8c">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/VisualVoice">code]</a> [<a target="_blank" rel="noopener" href="http://vision.cs.utexas.edu/projects/VisualVoice/VisualVoice_Supp.pdf">supplementary material]</a> *</li>
<li>M. Gogate, A. Adeel, R. Marxer, J. Barker, and A. Hussain, “DNN driven speaker independent audio-visual mask estimation for speech separation,” in Proc. of Interspeech, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.00060.pdf">paper]</a></li>
<li>M. Gogate, K. Dashtipour, A. Adeel, and A. Hussain, “Cochleanet: A robust language-independent audio-visual model for speech enhancement,” Information Fusion, vol. 63, pp. 273–285, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.10407.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://cochleanet.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://vimeo.com/357546330">demo]</a> [<a target="_blank" rel="noopener" href="https://cochleanet.github.io/supplementary/">supplementary material]</a></li>
<li>M. Gogate, K. Dashtipour, and A. Hussain, “Towards Robust Real-time Audio-Visual Speech Enhancement,” arXiv preprint arXiv:2112.09060, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.09060.pdf">paper]</a> *</li>
<li>R. Gu, S.-X. Zhang, Y. Xu, L. Chen, Y. Zou, and D. Yu, “Multi-modal multi-channel target speech separation,” IEEE Journal of Selected Topics in Signal Processing, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.07032.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://moplast.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://moplast.github.io/">demo]</a></li>
<li>J.-C. Hou, S.-S. Wang, Y.-H. Lai, Y. Tsao, H.-W. Chang, and H.- M. Wang, “Audio-visual speech enhancement using multimodal deep convolutional neural networks,” IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 2, no. 2, pp. 117–128, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.10893.pdf">paper]</a></li>
<li>J.-C. Hou, S.-S. Wang, Y.-H. Lai, J.-C. Lin, Y. Tsao, H.-W. Chang, and H.-M. Wang, “Audio-visual speech enhancement using deep neural networks,” in Proc. of APSIPA, 2016. [<a target="_blank" rel="noopener" href="https://www.citi.sinica.edu.tw/papers/yu.tsao/5427-F.pdf">paper]</a></li>
<li>A. Hussain, J. Barker, R. Marxer, A. Adeel, W. Whitmer, R. Watt, and P. Derleth, “Towards multi-modal hearing aid design and evaluation in realistic audio-visual settings: Challenges and opportunities,” in Proc. of CHAT, 2017. [<a target="_blank" rel="noopener" href="http://spandh.dcs.shef.ac.uk/chat2017/papers/CHAT_2017_hussain.pdf">paper]</a></li>
<li>T. Hussain, M. Gogate, K. Dashtipour, and A. Hussain, “Towards intelligibility-oriented audio-visual speech enhancement,” arXiv preprint arXiv:2111.09642, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.09642.pdf">paper]</a> *</li>
<li>E. Ideli, “Audio-visual speech processing using deep learning techniques.” MSc thesis, Applied Sciences: School of Engineering Science, 2019. [<a target="_blank" rel="noopener" href="https://summit.sfu.ca/item/19744">paper]</a></li>
<li>E. Ideli, B. Sharpe, I. V. Bajić, and R. G. Vaughan,“Visually assisted time-domain speech enhancement,” in Proc. of GlobalSIP, 2019. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8969244">paper]</a></li>
<li>B. İnan, M. Cernak, H. Grabner, H. P. Tukuljac, R. C. Pena, and B. Ricaud, “Evaluating audiovisual source separation in the context of video conferencing,” Proc. of Interspeech, 2019. [<a target="_blank" rel="noopener" href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2671.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/berkayinan/audiovisual-separation-for-vc">code]</a></li>
<li>K. Ito, M. Yamamoto, and K. Nagamatsu, “Audio-visual speech enhancement method conditioned in the lip motion and speaker-discriminative embeddings,” Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9414133">paper]</a> *</li>
<li>M. L. Iuzzolino and K. Koishida, “AV(SE)²: Audio-visual squeeze- excite speech enhancement,” in Proc. of ICASSP. IEEE, 2020, pp. 7539–7543. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9054528">paper]</a></li>
<li>H. R. V. Joze, A. Shaban, M. L. Iuzzolino, and K. Koishida, “MMTM: Multimodal transfer module for CNN fusion,” Proc. of CVPR, 2020. [<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Joze_MMTM_Multimodal_Transfer_Module_for_CNN_Fusion_CVPR_2020_paper.pdf">paper]</a></li>
<li>F. U. Khan, B. P. Milner, and T. Le Cornu, “Using visual speech information in masking methods for audio speaker separation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1742–1754, 2018. [<a target="_blank" rel="noopener" href="https://ueaeprints.uea.ac.uk/id/eprint/67404/1/ieee_speaker_separation_2015_v4.0.pdf">paper]</a></li>
<li>C. Li and Y. Qian, “Deep audio-visual speech separation with attention mechanism,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9054180">paper]</a></li>
<li>Y. Li, Z. Liu, Y. Na, Z. Wang, B. Tian, and Q. Fu, “A visual-pilot deep fusion for target speech separation in multitalker noisy environment,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9054263">paper]</a></li>
<li>R. Lu, Z. Duan, and C. Zhang, “Listen and look: Audio–visual matching assisted speech source separation,” IEEE Signal Processing Letters, vol. 25, no. 9, pp. 1315–1319, 2018. [<a target="_blank" rel="noopener" href="http://www2.ece.rochester.edu/projects/air/publications/lu2018listen.pdf">paper]</a></li>
<li>R. Lu, Z. Duan, and C. Zhang, “Audio–visual deep clustering for speech separation, ”IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 11, pp. 1697–1712, 2019. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8762221">paper]</a></li>
<li>Y. Luo, J. Wang, X. Wang, L. Wen, and L. Wang, “Audio-visual speech separation using i-Vectors,” in Proc. of ICICSP, 2019. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8958547">paper]</a></li>
<li>N. Makishima, M. Ihori, A. Takashima, T. Tanaka, S. Orihashi, and R. Masumura, “Audio-visual speech separation using cross-modal correspondence loss,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.01463.pdf">paper]</a> *</li>
<li>D. Michelsanti, Z.-H. Tan, S. Sigurdsson, and J. Jensen, “On training targets and objective functions for deep-learning-based audio-visual speech enhancement,” in Proc. of ICASSP, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.06234.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://kom.aau.dk/~zt/online/icassp2019_sup_mat.pdf">supplementary material]</a></li>
<li>D. Michelsanti, Z.-H. Tan, S. Sigurdsson, and J. Jensen, “Deep- learning-based audio-visual speech enhancement in presence of Lombard effect,” Speech Communication, vol. 115, pp. 38–50, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.12605.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=IRlaU0EMeOg">demo]</a></li>
<li>D. Michelsanti, Z.-H. Tan, S. Sigurdsson, and J. Jensen, “Effects of Lombard reflex on the performance of deep-learning-based audio-visual speech enhancement systems,” in Proc. of ICASSP, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.06250.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://vbn.aau.dk/en/activities/demo-effects-of-lombard-reflex-on-deep-learning-based-audio-visua">demo]</a></li>
<li>J. F. Montesinos, V. S. Kadandale, and G. Haro, “VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer,” arXiv preprint arXiv:2203.04099, 2022. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.04099.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://ipcv.github.io/VoViT/demos/">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/JuanFMontesinos/VoViT">code]</a> [<a target="_blank" rel="noopener" href="https://ipcv.github.io/VoViT/">project page]</a> *</li>
<li>G. Morrone, S. Bergamaschi, L. Pasa, L. Fadiga, V. Tikhanoff, and L. Badino, “Face landmark-based speaker-independent audio-visual speech enhancement in multi-talker environments,” in Proc. of ICASSP, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.02480.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://dr-pato.github.io/audio_visual_speech_enhancement/">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=YQ0q-OFphKM&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://dr-pato.github.io/audio_visual_speech_enhancement/">other demos]</a> [<a target="_blank" rel="noopener" href="https://github.com/dr-pato/audio_visual_speech_enhancement">code]</a></li>
<li>T. Ochiai, M. Delcroix, K. Kinoshita, A. Ogawa, and T. Nakatani, “Multimodal SpeakerBeam: Single channel target speech extraction with audio-visual speaker clues,” Proc. Interspeech, 2019. [<a target="_blank" rel="noopener" href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1513.pdf">paper]</a></li>
<li>A. Owens and A. A. Efros, “Audio-visual scene analysis with self-supervised multisensory features,” in Proc. of ECCV, 2018. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.03641.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://andrewowens.com/multisensory">project page]</a> [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=rwCIRu_hAJ8&feature=emb_title">demo]</a> [<a target="_blank" rel="noopener" href="https://github.com/andrewowens/multisensory">code]</a></li>
<li>Z. Pan, M. Ge and H. Li, “USEV: Universal speaker extraction with visual cue,” 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.14831.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/zexupan/USEV">code]</a> *</li>
<li>Z. Pan, R. Tao, C. Xu and H. Li, “MuSe: Multi-modal target speaker extraction with visual cues,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.07775.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/zexupan/MuSE">code]</a> *</li>
<li>Z. Pan, R. Tao, C. Xu and H. Li, “Selective Hearing through Lip-reading,” arXiv preprint arXiv:2106.07150, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.07150.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/zexupan/reentry">code]</a> *</li>
<li>L. Pasa, G. Morrone, and L. Badino, “An analysis of speech enhancement and recognition losses in limited resources multi-talker single channel audio-visual ASR,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.08248.pdf">paper]</a></li>
<li>L. Qu, C. Weber, and S. Wermter, “Multimodal target speech separation with voice and face references,” arXiv preprint arXiv:2005.08335, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.08335.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/danmic/av-se/blob/master/leyuanqu.github.io/INTERSPEECH2020">project page]</a> [<a target="_blank" rel="noopener" href="https://github.com/danmic/av-se/blob/master/leyuanqu.github.io/INTERSPEECH2020">demo]</a></li>
<li>M. Sadeghi and X. Alameda-Pineda, “Mixture of inference networks for VAE-based audio-visual speech enhancement,” arXiv preprint arXiv:1912.10647, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.10647.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/min-vae-se/">project page]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/min-vae-se/#audio">demo]</a> [<a target="_blank" rel="noopener" href="https://gitlab.inria.fr/smostafa/avse-vae">code]</a></li>
<li>M. Sadeghi and X. Alameda-Pineda, “Robust unsupervised audio-visual speech enhancement using a mixture of variational autoencoders,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.03930.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/vae-mm-se/">project page]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/files/2019/10/vae_mm_supp.pdf">supplementary material]</a> [<a target="_blank" rel="noopener" href="https://gitlab.inria.fr/smostafa/avse-vae">code]</a></li>
<li>M. Sadeghi and X. Alameda-Pineda, “Switching variational auto-encoders for noise-agnostic audio-visual speech enhancement,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.04144.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/swvae/">project page]</a> *</li>
<li>M. Sadeghi, S. Leglaive, X. Alameda-Pineda, L. Girin, and R. Horaud, “Audio-visual speech enhancement using conditional variational autoencoders,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 1788–1800, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.02590.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/av-vae-se/">project page]</a> [<a target="_blank" rel="noopener" href="https://team.inria.fr/perception/research/av-vae-se/">demo]</a> [<a target="_blank" rel="noopener" href="https://gitlab.inria.fr/smostafa/avse-vae">code]</a></li>
<li>H. Sato, T. Ochiai, K. Kinoshita, M. Delcroix, T. Nakatani and S. Araki. “Multimodal attention fusion for target speaker extraction,” in Proc. of SLT, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.01326.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.kecl.ntt.co.jp/icl/signal/member/demo/audio_visual_speakerbeam.html">project page]</a> [<a target="_blank" rel="noopener" href="http://www.kecl.ntt.co.jp/icl/signal/member/demo/audio_visual_speakerbeam.html">demo]</a> *</li>
<li>S. S. Shetu, S. Chakrabarty, and E. A. P. Habets, “An empirical study of visual features for DNN based audio-visual speech enhancement in multi-talker environments,” in Proc. of ICASSP, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.04359.pdf">paper]</a> *</li>
<li>Z. Sun, Y. Wang, and L. Cao, “An attention based speaker-independent audio-visual deep learning model for speech enhancement,” in Proc. of MMM, 2020. [<a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-030-37734-2_60">paper]</a></li>
<li>K. Tan, Y. Xu, S.-X. Zhang, M. Yu, and D. Yu, “Audio-visual speech separation and dereverberation with a two-stage multimodal network,” IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 3, pp. 542–553, 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.07352.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://jupiterethan.github.io/av-enh.github.io/">project page]</a> [<a target="_blank" rel="noopener" href="https://jupiterethan.github.io/av-enh.github.io/">demo]</a></li>
<li>W. Wang, C. Xing, D. Wang, X. Chen, and F. Sun, “A robust audio-visual speech enhancement model,” in Proc. of ICASSP, 2020. [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9053033">paper]</a></li>
<li>J. Wu, Y. Xu, S.-X. Zhang, L.-W. Chen, M. Yu, L. Xie, and D. Yu, “Time domain audio visual speech separation,” in Proc. of ASRU, 2019. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03760.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://funcwj.github.io/online-demo/page/tavs">project page]</a> [<a target="_blank" rel="noopener" href="https://funcwj.github.io/online-demo/page/tavs">demo]</a></li>
<li>Z. Wu, S. Sivadas, Y. K. Tan, M. Bin, and R. S. M. Goh,“Multi-modal hybrid deep neural network for speech enhancement,” arXiv preprint arXiv:1606.04750, 2016. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.04750.pdf">paper]</a></li>
<li>X. Xu, Y. Wang, D. Xu, C. Zhang, Y. Peng, J. Jia, and B. Chen, “VSEGAN: Visual speech enhancement generative adversarial network,” arXiv preprint arXiv:2102.02599, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.02599.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://xinmengxu.github.io/AVSE/VSEGAN">project page]</a> *</li>
<li>X. Xu, Y. Wang, D. Xu, C. Zhang, Y. Peng, J. Jia, and B. Chen, “AMFFCN: Attentional multi-layer feature fusion convolution network for audio-visual speech enhancement,” arXiv preprint arXiv:2101.06268, 2021. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.06268.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://xinmengxu.github.io/AVSE/AMFFCN">project page]</a> *</li>
<li>Y. Xu, M. Yu, S.-X. Zhang, L. Chen, C. Weng, J. Liu, and D. Yu, “Neural spatio-temporal beamformer for target speech separation,” Proc. of Interspeech (to appear), 2020. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.03889.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://yongxuustc.github.io/mtmvdr">project page]</a> [<a target="_blank" rel="noopener" href="https://yongxuustc.github.io/mtmvdr">demo]</a></li>
</ul>

	
	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/AVS增强-分离/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/EskimoA000.github.io/2022/03/15/Speech-Assessment/" >Speech Assessment</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2022-03-15  </div>
		</div>
	</div>
	


			<!--
<div class="entry">
  <div class="row">
	
	
		<h2 id="Performance-Assessment"><a href="#Performance-Assessment" class="headerlink" title="Performance Assessment"></a>Performance Assessment</h2><h3 id="Estimators-of-speech-quality-based-on-perceptual-models"><a href="#Estimators-of-speech-quality-based-on-perceptual-models" class="headerlink" title="Estimators of speech quality based on perceptual models"></a>Estimators of speech quality based on perceptual models</h3><ul>
<li>CSIG / CBAK / COVRL [<a target="_blank" rel="noopener" href="https://ecs.utdallas.edu/loizou/speech/obj_paper_jan08.pdf">paper]</a></li>
<li>HASQI [<a target="_blank" rel="noopener" href="https://www.aes.org/e-lib/browse.cfm?elib=15451">paper v1]</a> [<a target="_blank" rel="noopener" href="https://www.aes.org/e-lib/browse.cfm?elib=17126">paper v2]</a> [<a target="_blank" rel="noopener" href="https://www.colorado.edu/lab/hearlab/resources">code]</a></li>
<li>PESQ [<a target="_blank" rel="noopener" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.9136&rep=rep1&type=pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://www.itu.int/rec/T-REC-P.862-200511-I!Amd2/en">code]</a></li>
<li>POLQA [<a target="_blank" rel="noopener" href="https://www.itu.int/rec/T-REC-P.863-201803-I/en">recommendation]</a> [<a target="_blank" rel="noopener" href="http://www.polqa.info/products.html">code]</a></li>
<li>ViSQOL [<a target="_blank" rel="noopener" href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/39979.pdf">paper 1]</a> [<a target="_blank" rel="noopener" href="https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-015-0054-9">paper 2]</a> [<a target="_blank" rel="noopener" href="https://github.com/google/visqol">code]</a></li>
</ul>
<h3 id="Estimators-of-speech-quality-based-on-energy-ratios"><a href="#Estimators-of-speech-quality-based-on-energy-ratios" class="headerlink" title="Estimators of speech quality based on energy ratios"></a>Estimators of speech quality based on energy ratios</h3><ul>
<li>SDR / SIR / SAR (BSS Eval) [<a target="_blank" rel="noopener" href="https://www.irisa.fr/metiss/gribonval/Papers/2006/2006_IEEE_TSAP_VincentFevGrib.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://bass-db.gforge.inria.fr/bss_eval/">code]</a></li>
<li>SDI [<a target="_blank" rel="noopener" href="https://uol.de/f/6/dept/mediphysik/ag/sigproc/download/papers/doclo/journal_wiener.pdf">paper]</a></li>
<li>SI-SDR [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.02508.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/sigsep/bsseval/issues/3#issuecomment-494995846">code]</a></li>
</ul>
<h3 id="Estimators-of-speech-intelligibility"><a href="#Estimators-of-speech-intelligibility" class="headerlink" title="Estimators of speech intelligibility"></a>Estimators of speech intelligibility</h3><ul>
<li>CSII [<a target="_blank" rel="noopener" href="https://www.researchgate.net/profile/James_Kates2/publication/7842209_Coherence_and_the_speech_intelligibility_index/links/546f5dab0cf2d67fc0310f88/Coherence-and-the-speech-intelligibility-index.pdf">paper]</a></li>
<li>ESII [<a target="_blank" rel="noopener" href="https://pure.uva.nl/ws/files/3886153/45240_205638y.pdf">paper]</a></li>
<li>ESTOI [<a target="_blank" rel="noopener" href="https://www.researchgate.net/profile/Cees_Taal/publication/306046797_An_Algorithm_for_Predicting_the_Intelligibility_of_Speech_Masked_by_Modulated_Noise_Maskers/links/5ae0d5ab0f7e9b2859480a5e/An-Algorithm-for-Predicting-the-Intelligibility-of-Speech-Masked-by-Modulated-Noise-Maskers.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://kom.aau.dk/~jje/code/estoi.m">code]</a></li>
<li>HASPI [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0167639314000545">paper]</a> [<a target="_blank" rel="noopener" href="https://www.colorado.edu/lab/hearlab/resources">code]</a></li>
<li>SII [<a target="_blank" rel="noopener" href="https://global.ihs.com/doc_detail.cfm?document_name=ANSI/ASA%20S3.5&item_s_key=00009562&csf=ASA">paper]</a> [<a target="_blank" rel="noopener" href="http://sii.to/programs.html">code]</a></li>
<li>STOI [<a target="_blank" rel="noopener" href="http://cas.et.tudelft.nl/pubs/Taal2011_1.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://ceestaal.nl/stoi_mp.zip">code]</a></li>
</ul>

	

	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/Speech-Assessment/#more" class="btn btn-default more">阅读此文</a>
</div>

-->
<div class="entry">
  <div class="row">
	
	
		<h2 id="Performance-Assessment"><a href="#Performance-Assessment" class="headerlink" title="Performance Assessment"></a>Performance Assessment</h2><h3 id="Estimators-of-speech-quality-based-on-perceptual-models"><a href="#Estimators-of-speech-quality-based-on-perceptual-models" class="headerlink" title="Estimators of speech quality based on perceptual models"></a>Estimators of speech quality based on perceptual models</h3><ul>
<li>CSIG / CBAK / COVRL [<a target="_blank" rel="noopener" href="https://ecs.utdallas.edu/loizou/speech/obj_paper_jan08.pdf">paper]</a></li>
<li>HASQI [<a target="_blank" rel="noopener" href="https://www.aes.org/e-lib/browse.cfm?elib=15451">paper v1]</a> [<a target="_blank" rel="noopener" href="https://www.aes.org/e-lib/browse.cfm?elib=17126">paper v2]</a> [<a target="_blank" rel="noopener" href="https://www.colorado.edu/lab/hearlab/resources">code]</a></li>
<li>PESQ [<a target="_blank" rel="noopener" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.9136&rep=rep1&type=pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://www.itu.int/rec/T-REC-P.862-200511-I!Amd2/en">code]</a></li>
<li>POLQA [<a target="_blank" rel="noopener" href="https://www.itu.int/rec/T-REC-P.863-201803-I/en">recommendation]</a> [<a target="_blank" rel="noopener" href="http://www.polqa.info/products.html">code]</a></li>
<li>ViSQOL [<a target="_blank" rel="noopener" href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/39979.pdf">paper 1]</a> [<a target="_blank" rel="noopener" href="https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-015-0054-9">paper 2]</a> [<a target="_blank" rel="noopener" href="https://github.com/google/visqol">code]</a></li>
</ul>
<h3 id="Estimators-of-speech-quality-based-on-energy-ratios"><a href="#Estimators-of-speech-quality-based-on-energy-ratios" class="headerlink" title="Estimators of speech quality based on energy ratios"></a>Estimators of speech quality based on energy ratios</h3><ul>
<li>SDR / SIR / SAR (BSS Eval) [<a target="_blank" rel="noopener" href="https://www.irisa.fr/metiss/gribonval/Papers/2006/2006_IEEE_TSAP_VincentFevGrib.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://bass-db.gforge.inria.fr/bss_eval/">code]</a></li>
<li>SDI [<a target="_blank" rel="noopener" href="https://uol.de/f/6/dept/mediphysik/ag/sigproc/download/papers/doclo/journal_wiener.pdf">paper]</a></li>
<li>SI-SDR [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.02508.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/sigsep/bsseval/issues/3#issuecomment-494995846">code]</a></li>
</ul>
<h3 id="Estimators-of-speech-intelligibility"><a href="#Estimators-of-speech-intelligibility" class="headerlink" title="Estimators of speech intelligibility"></a>Estimators of speech intelligibility</h3><ul>
<li>CSII [<a target="_blank" rel="noopener" href="https://www.researchgate.net/profile/James_Kates2/publication/7842209_Coherence_and_the_speech_intelligibility_index/links/546f5dab0cf2d67fc0310f88/Coherence-and-the-speech-intelligibility-index.pdf">paper]</a></li>
<li>ESII [<a target="_blank" rel="noopener" href="https://pure.uva.nl/ws/files/3886153/45240_205638y.pdf">paper]</a></li>
<li>ESTOI [<a target="_blank" rel="noopener" href="https://www.researchgate.net/profile/Cees_Taal/publication/306046797_An_Algorithm_for_Predicting_the_Intelligibility_of_Speech_Masked_by_Modulated_Noise_Maskers/links/5ae0d5ab0f7e9b2859480a5e/An-Algorithm-for-Predicting-the-Intelligibility-of-Speech-Masked-by-Modulated-Noise-Maskers.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://kom.aau.dk/~jje/code/estoi.m">code]</a></li>
<li>HASPI [<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0167639314000545">paper]</a> [<a target="_blank" rel="noopener" href="https://www.colorado.edu/lab/hearlab/resources">code]</a></li>
<li>SII [<a target="_blank" rel="noopener" href="https://global.ihs.com/doc_detail.cfm?document_name=ANSI/ASA%20S3.5&item_s_key=00009562&csf=ASA">paper]</a> [<a target="_blank" rel="noopener" href="http://sii.to/programs.html">code]</a></li>
<li>STOI [<a target="_blank" rel="noopener" href="http://cas.et.tudelft.nl/pubs/Taal2011_1.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://ceestaal.nl/stoi_mp.zip">code]</a></li>
</ul>

	
	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/Speech-Assessment/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/EskimoA000.github.io/2022/03/15/AV-Speech-Corpora/" >AV Speech Corpora</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2022-03-15  </div>
		</div>
	</div>
	


			<!--
<div class="entry">
  <div class="row">
	
	
		<h2 id="Audio-Visual-Speech-Corpora"><a href="#Audio-Visual-Speech-Corpora" class="headerlink" title="Audio-Visual Speech Corpora"></a>Audio-Visual Speech Corpora</h2><ul>
<li>AVA-ActiveSpeaker [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.01342.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://research.google.com/ava/index.html">dataset page]</a></li>
<li>AV Chinese Mandarin [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.07352.pdf">paper]</a></li>
<li>AVSpeech [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.03619.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://looking-to-listen.github.io/avspeech/index.html">dataset page]</a></li>
<li>ASPIRE [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.10407.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://cochleanet.github.io/">dataset page]</a></li>
<li>CREMA-D [<a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313618/">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/CheyneyComputerScience/CREMA-D">dataset page]</a> *</li>
<li>CUAVE [<a target="_blank" rel="noopener" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.6375&rep=rep1&type=pdf">paper]</a></li>
<li>EasyCom [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.04174.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/EasyComDataset">dataset page]</a> *</li>
<li>GRID [<a target="_blank" rel="noopener" href="http://www.laslab.org/wp-content/uploads/an_audio-visual_corpus_for_speech_perception_and_automatic_speech_recognition.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://spandh.dcs.shef.ac.uk/gridcorpus/">dataset page]</a></li>
<li>KinectDigits [<a target="_blank" rel="noopener" href="https://www.honda-ri.de/pubs/pdf/3275.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://zenodo.org/record/823531#.Xzml9y17HOQ">dataset page]</a></li>
<li>LDC2009V01 [<a target="_blank" rel="noopener" href="https://catalog.ldc.upenn.edu/LDC2009V01">dataset page]</a></li>
<li>Lombard GRID [<a target="_blank" rel="noopener" href="https://staffwww.dcs.shef.ac.uk/people/G.Brown/pdf/alghamdi_etal_2018_lombard.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://spandh.dcs.shef.ac.uk/avlombard/">dataset page]</a></li>
<li>LRS [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/publications/2017/Chung17/chung17.pdf">paper]</a></li>
<li>LRS2 [<a target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~vgg/publications/2019/Afouras19/afouras18c.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html">dataset page]</a></li>
<li>LRS3 [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.00496.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs3.html">dataset page]</a></li>
<li>LRW [<a target="_blank" rel="noopener" href="https://ora.ox.ac.uk/objects/uuid:c3238375-ec8b-4ecd-9543-8b179a6b74ba/download_file?safe_filename=chung16.pdf&file_format=application/pdf&type_of_work=Conference+item">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html">dataset page]</a></li>
<li>Mandarin Sentences Corpus [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.10893">paper]</a></li>
<li>MODALITY [<a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s10844-016-0438-z">paper]</a> [<a target="_blank" rel="noopener" href="http://modality-corpus.org/">dataset page]</a></li>
<li>MV-LRS [<a target="_blank" rel="noopener" href="https://ora.ox.ac.uk/objects/uuid:9f06858c-349c-416f-8ace-87751cd401fc/download_file?safe_filename=chung17a.pdf&file_format=application/pdf&type_of_work=Conference+item">paper]</a></li>
<li>NTCD-TIMIT [<a target="_blank" rel="noopener" href="https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0860.PDF">paper]</a> [<a target="_blank" rel="noopener" href="https://zenodo.org/record/260228#.XzqK5y17HOQ">dataset page]</a></li>
<li>Obama Weekly Addresses [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.08789">paper]</a></li>
<li>OuluVS [<a target="_blank" rel="noopener" href="http://www.ee.oulu.fi/~gyzhao/Papers/2009/Lipreading%20with%20Local%20Spatiotemporal.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://www.oulu.fi/cmvs/node/41315">dataset page]</a></li>
<li>OuluVS2 [<a target="_blank" rel="noopener" href="https://www.researchgate.net/profile/Ziheng_Zhou/publication/283593688_OuluVS2_A_multi-view_audiovisual_database_for_non-rigid_mouth_motion_analysis/links/5754caf608ae17e65ecccde3.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.ee.oulu.fi/research/imag/OuluVS2/">dataset page]</a></li>
<li>RAVDESS [<a target="_blank" rel="noopener" href="https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0196391&type=printable">paper]</a> [<a target="_blank" rel="noopener" href="https://zenodo.org/record/1188976#.XzskXy17HOQ">dataset page]</a></li>
<li>Small Mandarin Sentences Corpus [<a target="_blank" rel="noopener" href="https://www.citi.sinica.edu.tw/papers/yu.tsao/5427-F.pdf">paper]</a></li>
<li>TCD-TIMIT [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/7050271">paper]</a> [<a target="_blank" rel="noopener" href="https://sigmedia.tcd.ie/TCDTIMIT/">dataset page]</a></li>
<li>VoxCeleb [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.08612">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html">dataset page]</a></li>
<li>VoxCeleb2 [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.05622">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html">dataset page]</a></li>
</ul>

	

	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/AV-Speech-Corpora/#more" class="btn btn-default more">阅读此文</a>
</div>

-->
<div class="entry">
  <div class="row">
	
	
		<h2 id="Audio-Visual-Speech-Corpora"><a href="#Audio-Visual-Speech-Corpora" class="headerlink" title="Audio-Visual Speech Corpora"></a>Audio-Visual Speech Corpora</h2><ul>
<li>AVA-ActiveSpeaker [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.01342.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://research.google.com/ava/index.html">dataset page]</a></li>
<li>AV Chinese Mandarin [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.07352.pdf">paper]</a></li>
<li>AVSpeech [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.03619.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://looking-to-listen.github.io/avspeech/index.html">dataset page]</a></li>
<li>ASPIRE [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.10407.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://cochleanet.github.io/">dataset page]</a></li>
<li>CREMA-D [<a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313618/">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/CheyneyComputerScience/CREMA-D">dataset page]</a> *</li>
<li>CUAVE [<a target="_blank" rel="noopener" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.6375&rep=rep1&type=pdf">paper]</a></li>
<li>EasyCom [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.04174.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/EasyComDataset">dataset page]</a> *</li>
<li>GRID [<a target="_blank" rel="noopener" href="http://www.laslab.org/wp-content/uploads/an_audio-visual_corpus_for_speech_perception_and_automatic_speech_recognition.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://spandh.dcs.shef.ac.uk/gridcorpus/">dataset page]</a></li>
<li>KinectDigits [<a target="_blank" rel="noopener" href="https://www.honda-ri.de/pubs/pdf/3275.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://zenodo.org/record/823531#.Xzml9y17HOQ">dataset page]</a></li>
<li>LDC2009V01 [<a target="_blank" rel="noopener" href="https://catalog.ldc.upenn.edu/LDC2009V01">dataset page]</a></li>
<li>Lombard GRID [<a target="_blank" rel="noopener" href="https://staffwww.dcs.shef.ac.uk/people/G.Brown/pdf/alghamdi_etal_2018_lombard.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://spandh.dcs.shef.ac.uk/avlombard/">dataset page]</a></li>
<li>LRS [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/publications/2017/Chung17/chung17.pdf">paper]</a></li>
<li>LRS2 [<a target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~vgg/publications/2019/Afouras19/afouras18c.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html">dataset page]</a></li>
<li>LRS3 [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.00496.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs3.html">dataset page]</a></li>
<li>LRW [<a target="_blank" rel="noopener" href="https://ora.ox.ac.uk/objects/uuid:c3238375-ec8b-4ecd-9543-8b179a6b74ba/download_file?safe_filename=chung16.pdf&file_format=application/pdf&type_of_work=Conference+item">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html">dataset page]</a></li>
<li>Mandarin Sentences Corpus [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.10893">paper]</a></li>
<li>MODALITY [<a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s10844-016-0438-z">paper]</a> [<a target="_blank" rel="noopener" href="http://modality-corpus.org/">dataset page]</a></li>
<li>MV-LRS [<a target="_blank" rel="noopener" href="https://ora.ox.ac.uk/objects/uuid:9f06858c-349c-416f-8ace-87751cd401fc/download_file?safe_filename=chung17a.pdf&file_format=application/pdf&type_of_work=Conference+item">paper]</a></li>
<li>NTCD-TIMIT [<a target="_blank" rel="noopener" href="https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0860.PDF">paper]</a> [<a target="_blank" rel="noopener" href="https://zenodo.org/record/260228#.XzqK5y17HOQ">dataset page]</a></li>
<li>Obama Weekly Addresses [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.08789">paper]</a></li>
<li>OuluVS [<a target="_blank" rel="noopener" href="http://www.ee.oulu.fi/~gyzhao/Papers/2009/Lipreading%20with%20Local%20Spatiotemporal.pdf">paper]</a> [<a target="_blank" rel="noopener" href="https://www.oulu.fi/cmvs/node/41315">dataset page]</a></li>
<li>OuluVS2 [<a target="_blank" rel="noopener" href="https://www.researchgate.net/profile/Ziheng_Zhou/publication/283593688_OuluVS2_A_multi-view_audiovisual_database_for_non-rigid_mouth_motion_analysis/links/5754caf608ae17e65ecccde3.pdf">paper]</a> [<a target="_blank" rel="noopener" href="http://www.ee.oulu.fi/research/imag/OuluVS2/">dataset page]</a></li>
<li>RAVDESS [<a target="_blank" rel="noopener" href="https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0196391&type=printable">paper]</a> [<a target="_blank" rel="noopener" href="https://zenodo.org/record/1188976#.XzskXy17HOQ">dataset page]</a></li>
<li>Small Mandarin Sentences Corpus [<a target="_blank" rel="noopener" href="https://www.citi.sinica.edu.tw/papers/yu.tsao/5427-F.pdf">paper]</a></li>
<li>TCD-TIMIT [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/7050271">paper]</a> [<a target="_blank" rel="noopener" href="https://sigmedia.tcd.ie/TCDTIMIT/">dataset page]</a></li>
<li>VoxCeleb [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.08612">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html">dataset page]</a></li>
<li>VoxCeleb2 [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.05622">paper]</a> [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html">dataset page]</a></li>
</ul>

	
	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/AV-Speech-Corpora/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/EskimoA000.github.io/2022/03/15/读书杂谈-暗知识/" >读书杂谈-暗知识(持续更新)</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2022-03-15  </div>
		</div>
	</div>
	


			<!--
<div class="entry">
  <div class="row">
	
	
		<p>暗知识—你的认知正在阻碍你  [英]Michael Blastland著</p>
<p>伟大的意识形态是建立在关于基因与环境、先天与后天的相互矛盾的观念之上。</p>
<p>人们在面对“偶然性”时可能太过听天由命，同时在面对“噪音”时又太不屑一顾。</p>
<p>无须陷入虚无主义的绝望中</p>
<p>对进步的最大威胁，不是无知，而是对已有或未知知识的错觉。我们迫切需要摒弃一些错觉，这样才能将前路看的更加清晰。</p>
<p>犬儒主义：主张以追求普遍的善为人生之目的，为此必须抛弃一切物质享受和感官快乐。</p>
<p>人生需要回顾反思，但要想过得更好，我们更要积极向前看。</p>
<p>情景选择：若干不同挺像因素混杂在一起共同作用</p>

	

	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/读书杂谈-暗知识/#more" class="btn btn-default more">阅读此文</a>
</div>

-->
<div class="entry">
  <div class="row">
	
	
		<p>暗知识—你的认知正在阻碍你  [英]Michael Blastland著</p>
<p>伟大的意识形态是建立在关于基因与环境、先天与后天的相互矛盾的观念之上。</p>
<p>人们在面对“偶然性”时可能太过听天由命，同时在面对“噪音”时又太不屑一顾。</p>
<p>无须陷入虚无主义的绝望中</p>
<p>对进步的最大威胁，不是无知，而是对已有或未知知识的错觉。我们迫切需要摒弃一些错觉，这样才能将前路看的更加清晰。</p>
<p>犬儒主义：主张以追求普遍的善为人生之目的，为此必须抛弃一切物质享受和感官快乐。</p>
<p>人生需要回顾反思，但要想过得更好，我们更要积极向前看。</p>
<p>情景选择：若干不同挺像因素混杂在一起共同作用</p>

	
	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/读书杂谈-暗知识/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/EskimoA000.github.io/2022/03/15/cocktail/" >cocktail</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2022-03-15  </div>
		</div>
	</div>
	


			<!--
<div class="entry">
  <div class="row">
	
	
		<p>Looking to Listen at the Cocktail Party:</p>
<p>A Speaker-Independent Audio-Visual Model for Speech Separation</p>
<p><em>前言</em></p>
<p>在嘈杂的环境中，人们非常善于把注意力集中在某个特定的人身上，在心理上“屏蔽”其他所有声音。这种能力被称为“鸡尾酒会效应”。</p>
<p> “语音分离”（Speech Separation）来自于“鸡尾酒会问题”，采集的音频信号中除了主讲话者之外，还有其他人说话声的干扰和噪音干扰。语音分离的目标就是从这些干扰中分离出主讲话者的语音。</p>
<p>根据干扰的不同，语音分离任务可以分为三类：</p>
<p>·         当干扰为噪声信号时，可以称为“语音增强”（Speech Enhancement）</p>
<p>·         当干扰为其他讲话者时，可以称为“多讲话者分离”（Speaker Separation）</p>
<p>·         当干扰为目标讲话者自己声音的反射波时，可以称为“解混响”（De-reverberation）</p>
	

	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/cocktail/#more" class="btn btn-default more">阅读此文</a>
</div>

-->
<div class="entry">
  <div class="row">
	
	
		<p>Looking to Listen at the Cocktail Party:</p>
<p>A Speaker-Independent Audio-Visual Model for Speech Separation</p>
<p><em>前言</em></p>
<p>在嘈杂的环境中，人们非常善于把注意力集中在某个特定的人身上，在心理上“屏蔽”其他所有声音。这种能力被称为“鸡尾酒会效应”。</p>
<p> “语音分离”（Speech Separation）来自于“鸡尾酒会问题”，采集的音频信号中除了主讲话者之外，还有其他人说话声的干扰和噪音干扰。语音分离的目标就是从这些干扰中分离出主讲话者的语音。</p>
<p>根据干扰的不同，语音分离任务可以分为三类：</p>
<p>·         当干扰为噪声信号时，可以称为“语音增强”（Speech Enhancement）</p>
<p>·         当干扰为其他讲话者时，可以称为“多讲话者分离”（Speaker Separation）</p>
<p>·         当干扰为目标讲话者自己声音的反射波时，可以称为“解混响”（De-reverberation）</p>
	
	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/cocktail/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/EskimoA000.github.io/2022/03/15/test/" >图片功能TEST</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2022-03-15  </div>
		</div>
	</div>
	


			<!--
<div class="entry">
  <div class="row">
	
	
		<p><img src="/EskimoA000.github.io/2022/03/15/test/ac.jpg"></p>

	

	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/test/#more" class="btn btn-default more">阅读此文</a>
</div>

-->
<div class="entry">
  <div class="row">
	
	
		<p><img src="/EskimoA000.github.io/2022/03/15/test/ac.jpg"></p>

	
	</div>
  <a type="button" href="/EskimoA000.github.io/2022/03/15/test/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/EskimoA000.github.io/2021/10/22/多模态信息处理-语言表征-词表征/" >多模态信息处理-语言表征(词表征)</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2021-10-22  </div>
		</div>
	</div>
	


			<!--
<div class="entry">
  <div class="row">
	
	
		<h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>印欧语系是否需要分词？ 如果每一个词当作一个单词，没有联系（computer computers）   空格切分导致数据稀疏</p>
<p>英语 子词切分-将一个单词切分为若干连续的片段（bert 输入） 贪心算法</p>
<p>BPE（byte pair encoding）子词词表构建  初始为所有的字符‘a’‘b’…</p>
<p>编码 解码</p>
<h3 id="独热表征"><a href="#独热表征" class="headerlink" title="独热表征"></a>独热表征</h3><p>高维、稀疏、离散 所有向量都是正交的</p>
<p>优化：</p>
<p> 增加额外的特征：n v adj  加前后缀的特征</p>
	

	</div>
  <a type="button" href="/EskimoA000.github.io/2021/10/22/多模态信息处理-语言表征-词表征/#more" class="btn btn-default more">阅读此文</a>
</div>

-->
<div class="entry">
  <div class="row">
	
	
		<h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>印欧语系是否需要分词？ 如果每一个词当作一个单词，没有联系（computer computers）   空格切分导致数据稀疏</p>
<p>英语 子词切分-将一个单词切分为若干连续的片段（bert 输入） 贪心算法</p>
<p>BPE（byte pair encoding）子词词表构建  初始为所有的字符‘a’‘b’…</p>
<p>编码 解码</p>
<h3 id="独热表征"><a href="#独热表征" class="headerlink" title="独热表征"></a>独热表征</h3><p>高维、稀疏、离散 所有向量都是正交的</p>
<p>优化：</p>
<p> 增加额外的特征：n v adj  加前后缀的特征</p>
	
	</div>
  <a type="button" href="/EskimoA000.github.io/2021/10/22/多模态信息处理-语言表征-词表征/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/EskimoA000.github.io/2021/10/15/多模态信息处理-图像表征/" >多模态信息处理-图像表征</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2021-10-15  </div>
		</div>
	</div>
	


			<!--
<div class="entry">
  <div class="row">
	
	
		<h2 id="图像表征-离散表征"><a href="#图像表征-离散表征" class="headerlink" title="图像表征-离散表征"></a>图像表征-离散表征</h2><p>dense向量表示的特征最多 先验就是生成</p>
<p>生成图像–离散特征最好</p>
<h4 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ-VAE"></a>VQ-VAE</h4><p>quantization量化</p>
<p>自编码器 输入层、表征层（限制器）、重构层（重构输入）</p>
<p>量化过程，把卷积层的特征 channel聚类  自编码器加聚类</p>
<p>编码器如何做下采样。。</p>
<p>embedding space（dictionary） 相当于字典  最近的找到index 再还原回来</p>
	

	</div>
  <a type="button" href="/EskimoA000.github.io/2021/10/15/多模态信息处理-图像表征/#more" class="btn btn-default more">阅读此文</a>
</div>

-->
<div class="entry">
  <div class="row">
	
	
		<h2 id="图像表征-离散表征"><a href="#图像表征-离散表征" class="headerlink" title="图像表征-离散表征"></a>图像表征-离散表征</h2><p>dense向量表示的特征最多 先验就是生成</p>
<p>生成图像–离散特征最好</p>
<h4 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ-VAE"></a>VQ-VAE</h4><p>quantization量化</p>
<p>自编码器 输入层、表征层（限制器）、重构层（重构输入）</p>
<p>量化过程，把卷积层的特征 channel聚类  自编码器加聚类</p>
<p>编码器如何做下采样。。</p>
<p>embedding space（dictionary） 相当于字典  最近的找到index 再还原回来</p>
	
	</div>
  <a type="button" href="/EskimoA000.github.io/2021/10/15/多模态信息处理-图像表征/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/EskimoA000.github.io/2021/10/13/JMBook-Chapter-7-Reading/" >JMBook Chapter 7 Reading</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2021-10-13  </div>
		</div>
	</div>
	


			<!--
<div class="entry">
  <div class="row">
	
	
		<p>JMBook Chapter 7 Reading-神经网络与神经语言模型</p>
<p>神经网络是语言处理的基本计算工具，“神经”起源于McCulloch-Pitts神经元，这是一种简化的人类神经元模型，是一种可以用命题逻辑描述的计算元素。</p>
<p>共享大部分相同的数学表达。但神经网络是一种比逻辑回归更强大的分类器，技术上具有单个“隐藏层”的最小的神经网络可以被用于学习任何函数；通过逻辑回归，通过基于领域知识开发了许多丰富的特征模板，将回归分类器应用于许多不同的任务，使用神经网络时，避免使用大量的手工提取功能，而是建立神经网络，将原始单词作为输入并学习推导特征作为学习分类过程的一部分；这章介绍作为分类器的前馈网络，并将其应用于语言建模的简单任务：为单词序列分配概率和预测即将出现的单词。</p>
<p>7.1 Units</p>
<p>一个神经网络块的构建是单个计算单元。 单元将实数值作为输入，对它们执行一些计算，并产生一个输出。i神经元对输入进行加权求和，额外项称为偏差项(Bias term):</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png" alt="https://pic4.zhimg.com/80/v2-ccefaa5185bc1acf24645e39497bc27b_720w.jpg"></p>
<p>用矢量（vector）符号表示这个加权和：将权重向量w，标量偏差b和输入向量x来讨论z，用点积替换总和:</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png" alt="https://pic4.zhimg.com/80/v2-ac4e33e1c427f9c6dbd1e2da29abf91f_720w.jpg"></p>
<p>神经单元将非线性函数f应用于z。将f的输出称为激活单元的激活值a:</p>
	

	</div>
  <a type="button" href="/EskimoA000.github.io/2021/10/13/JMBook-Chapter-7-Reading/#more" class="btn btn-default more">阅读此文</a>
</div>

-->
<div class="entry">
  <div class="row">
	
	
		<p>JMBook Chapter 7 Reading-神经网络与神经语言模型</p>
<p>神经网络是语言处理的基本计算工具，“神经”起源于McCulloch-Pitts神经元，这是一种简化的人类神经元模型，是一种可以用命题逻辑描述的计算元素。</p>
<p>共享大部分相同的数学表达。但神经网络是一种比逻辑回归更强大的分类器，技术上具有单个“隐藏层”的最小的神经网络可以被用于学习任何函数；通过逻辑回归，通过基于领域知识开发了许多丰富的特征模板，将回归分类器应用于许多不同的任务，使用神经网络时，避免使用大量的手工提取功能，而是建立神经网络，将原始单词作为输入并学习推导特征作为学习分类过程的一部分；这章介绍作为分类器的前馈网络，并将其应用于语言建模的简单任务：为单词序列分配概率和预测即将出现的单词。</p>
<p>7.1 Units</p>
<p>一个神经网络块的构建是单个计算单元。 单元将实数值作为输入，对它们执行一些计算，并产生一个输出。i神经元对输入进行加权求和，额外项称为偏差项(Bias term):</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png" alt="https://pic4.zhimg.com/80/v2-ccefaa5185bc1acf24645e39497bc27b_720w.jpg"></p>
<p>用矢量（vector）符号表示这个加权和：将权重向量w，标量偏差b和输入向量x来讨论z，用点积替换总和:</p>
<p><img src="file:///C:/Users/ESKIMO~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png" alt="https://pic4.zhimg.com/80/v2-ac4e33e1c427f9c6dbd1e2da29abf91f_720w.jpg"></p>
<p>神经单元将非线性函数f应用于z。将f的输出称为激活单元的激活值a:</p>
	
	</div>
  <a type="button" href="/EskimoA000.github.io/2021/10/13/JMBook-Chapter-7-Reading/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/EskimoA000.github.io/2021/09/24/多模态信息处理-1/" >多模态信息处理-图像表征</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2021-09-24  </div>
		</div>
	</div>
	


			<!--
<div class="entry">
  <div class="row">
	
	
		<h1 id="L2-基于卷积神经网络的图像分类模型"><a href="#L2-基于卷积神经网络的图像分类模型" class="headerlink" title="L2 基于卷积神经网络的图像分类模型"></a>L2 基于卷积神经网络的图像分类模型</h1><h2 id="一-卷积层"><a href="#一-卷积层" class="headerlink" title="一.卷积层"></a>一.卷积层</h2><p>3通道RGB3<em>32</em>32 3072浮点型  全连接神经网络，加神经元 训练集提高，测试集下降  过拟合  原因是第一层时参数过多 1024*3072</p>
<p>初级视皮质 单个视觉细胞仅对部分区域的特定模式反应  </p>
<p>小区域检测模式，卷积神经网络</p>
<p>全图找特征，全连接神经网络  全连接层展开成一维</p>
<p>卷积的性质，（1）部分（鸟嘴）（2）相同的模式可能出现在图像不区域</p>
<p>卷积：局部特征提取器  对每个“子区域”进行加权求和 对每个“子区域”进行加权求和  卷积核：weight/filter/kernel</p>
<p>𝐖𝟏𝐱𝐇𝟏 —— 𝐅𝒘𝐱𝐅𝒉 ——𝐖𝟐𝐱𝐇𝟐  𝐖𝟐=𝐖𝟏−𝐅𝒘+𝟏  𝐇𝟐=𝐇𝟏−𝐅𝒉+𝟏</p>
	

	</div>
  <a type="button" href="/EskimoA000.github.io/2021/09/24/多模态信息处理-1/#more" class="btn btn-default more">阅读此文</a>
</div>

-->
<div class="entry">
  <div class="row">
	
	
		<h1 id="L2-基于卷积神经网络的图像分类模型"><a href="#L2-基于卷积神经网络的图像分类模型" class="headerlink" title="L2 基于卷积神经网络的图像分类模型"></a>L2 基于卷积神经网络的图像分类模型</h1><h2 id="一-卷积层"><a href="#一-卷积层" class="headerlink" title="一.卷积层"></a>一.卷积层</h2><p>3通道RGB3<em>32</em>32 3072浮点型  全连接神经网络，加神经元 训练集提高，测试集下降  过拟合  原因是第一层时参数过多 1024*3072</p>
<p>初级视皮质 单个视觉细胞仅对部分区域的特定模式反应  </p>
<p>小区域检测模式，卷积神经网络</p>
<p>全图找特征，全连接神经网络  全连接层展开成一维</p>
<p>卷积的性质，（1）部分（鸟嘴）（2）相同的模式可能出现在图像不区域</p>
<p>卷积：局部特征提取器  对每个“子区域”进行加权求和 对每个“子区域”进行加权求和  卷积核：weight/filter/kernel</p>
<p>𝐖𝟏𝐱𝐇𝟏 —— 𝐅𝒘𝐱𝐅𝒉 ——𝐖𝟐𝐱𝐇𝟐  𝐖𝟐=𝐖𝟏−𝐅𝒘+𝟏  𝐇𝟐=𝐇𝟏−𝐅𝒉+𝟏</p>
	
	</div>
  <a type="button" href="/EskimoA000.github.io/2021/09/24/多模态信息处理-1/#more" class="btn btn-default more">阅读此文</a>
</div>

		

		</div>

		<!-- pagination -->
		<div>
  		<center>
		<div class="pagination">

   
    
           <a type="button" class="btn btn-default disabled"><i class="fa fa-arrow-circle-o-left"></i>上一页</a>
        

        <a href="/EskimoA000.github.io/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
 
       <a href="/EskimoA000.github.io/page/2/" type="button" class="btn btn-default ">下一页<i class="fa fa-arrow-circle-o-right"></i></a>     
        

  
</div>

  		</center>
		</div>

		
		
	</div> <!-- col-md-9 -->

	
		<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="搜索" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			
	<div class="widget">
		<h4>分类</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/EskimoA000.github.io/categories/哲学/">哲学<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/categories/多模态/">多模态<span>4</span></a></li>
		
			<li><a href="/EskimoA000.github.io/categories/多模态语音/">多模态语音<span>2</span></a></li>
		
			<li><a href="/EskimoA000.github.io/categories/多模态语音降噪与分离/">多模态语音降噪与分离<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/categories/无题/">无题<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/categories/机器学习/">机器学习<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/categories/计算语言学/">计算语言学<span>2</span></a></li>
		
		</ul>
	</div>

		
			
	<div class="widget">
		<h4>标签云</h4>
		<ul class="tag_box inline list-unstyled">		
		
			<li><a href="/EskimoA000.github.io/tags/N元语法/">N元语法<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/tags/多模态语音降噪与分离论文集/">多模态语音降噪与分离论文集<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/tags/鸡尾酒会/">鸡尾酒会<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/tags/TEST/">TEST<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/tags/多模态语音数据集/">多模态语音数据集<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/tags/语音结果评价标准/">语音结果评价标准<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/tags/高级机器学习/">高级机器学习<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/tags/无题/">无题<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/tags/多模态/">多模态<span>4</span></a></li>
		
			<li><a href="/EskimoA000.github.io/tags/神经网络与神经语言模型/">神经网络与神经语言模型<span>1</span></a></li>
		
			<li><a href="/EskimoA000.github.io/tags/读书杂谈/">读书杂谈<span>1</span></a></li>
		
		 
		</ul>
	</div>


		
			
<div class="widget">
  <h4>最新文章</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/EskimoA000.github.io/2022/03/15/AVS增强-分离/" ><i class="fa fa-file-o"></i>AVS增强&amp;分离论文list</a>
      </li>
    
      <li>
        <a href="/EskimoA000.github.io/2022/03/15/Speech-Assessment/" ><i class="fa fa-file-o"></i>Speech Assessment</a>
      </li>
    
      <li>
        <a href="/EskimoA000.github.io/2022/03/15/AV-Speech-Corpora/" ><i class="fa fa-file-o"></i>AV Speech Corpora</a>
      </li>
    
      <li>
        <a href="/EskimoA000.github.io/2022/03/15/读书杂谈-暗知识/" ><i class="fa fa-file-o"></i>读书杂谈-暗知识(持续更新)</a>
      </li>
    
      <li>
        <a href="/EskimoA000.github.io/2022/03/15/cocktail/" ><i class="fa fa-file-o"></i>cocktail</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget">
	<h4>链接</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="https://github.com/EskimoA000" title="My Github account." target="_blank"]);">Github</a></li>
	
		<li><i class="fa fa-envelope-o"></i><a href="https://mail.bupt.edu.cn" title="My Email" target="_blank"]);">北邮人邮箱</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->

	
	
</div> <!-- row-fluid -->
	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  
  &copy; 2022 张靖元's Blog
  
      powered by <a href="http://hexo.io/" target="_blank">Hexo</a>.Theme <a href="https://github.com/Ares-X/hexo-theme-freemind.bithack" target="_blank">freemind.bithack</a>  
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/EskimoA000.github.io/js/jquery.imagesloaded.min.js"></script>
<script src="/EskimoA000.github.io/js/gallery.js"></script>
<script src="/EskimoA000.github.io/js/bootstrap.min.js"></script>
<script src="/EskimoA000.github.io/js/main.js"></script>
<script src="/EskimoA000.github.io/js/search.js"></script> 


<link rel="stylesheet" href="/EskimoA000.github.io/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/EskimoA000.github.io/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/EskimoA000.github.io/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

<script src="/EskimoA000.github.io/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/EskimoA000.github.io/live2dw/assets/assets/wanko.model.json"},"display":{"position":"right","width":260,"height":600},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.8},"log":false});</script></body>
   </html>
